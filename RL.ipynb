{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Introduction to RL for Game AI](https://i.imgur.com/FFiOMJo.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LINKS: https://tinyurl.com/UUAIRL\n",
    "\n",
    "## What Is Reinforcement Learning?\n",
    "\n",
    "Imagine teaching someone to play a video game without being able to tell them the rules. You can only give them a thumbs up when they do something good and a thumbs down when they do something bad. Over time, they'd figure out what works and what doesn't through trial and error.\n",
    "\n",
    "That's essentially what reinforcement learning (RL) is - a way for AI to learn by interacting with an environment and receiving feedback.\n",
    "\n",
    "### Today we will use Pong as a case study\n",
    "\n",
    "In our case, we're teaching an AI to play Pong by letting it:\n",
    "- Try different paddle movements\n",
    "- See what happens in the game\n",
    "- Get rewards for hitting the ball\n",
    "- Get penalties for missing the ball\n",
    "- Gradually improve its strategy through experience\n",
    "\n",
    "## The Key Components\n",
    "\n",
    "Let's break down the essential parts of our reinforcement learning system:\n",
    "\n",
    "1. **Agent**: The AI that controls the paddle\n",
    "2. **Environment**: The Pong game\n",
    "3. **State**: What our agent can observe about the game\n",
    "   - Ball x-position\n",
    "   - Ball y-position\n",
    "   - Paddle y-position\n",
    "   - Ball x-velocity\n",
    "   - Ball y-velocity\n",
    "4. **Actions**: What our agent can do\n",
    "   - Move paddle up\n",
    "   - Stay in place\n",
    "   - Move paddle down\n",
    "5. **Reward**: The feedback our agent receives\n",
    "   - Positive reward (+1) for hitting the ball\n",
    "   - Negative reward (-1) for missing the ball\n",
    "   - Small \"shaping\" rewards to guide learning\n",
    "\n",
    "\n",
    "![RL](https://upload.wikimedia.org/wikipedia/commons/1/1b/Reinforcement_learning_diagram.svg)\n",
    "(image from wikimedia)\n",
    "\n",
    "## The Learning Loop\n",
    "\n",
    "Here's how the learning process works:\n",
    "\n",
    "1. The agent observes the current state of the game\n",
    "2. Based on this state, it chooses an action (move up, stay, or move down)\n",
    "3. The game updates (the ball and paddle move)\n",
    "4. The agent receives a reward\n",
    "5. The agent observes the new state\n",
    "6. Repeat until the game ends\n",
    "7. After the game ends, the agent learns from what happened\n",
    "\n",
    "This cycle happens over and over - thousands of times - as the agent gradually improves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUT HOW DO WE DO THIS?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to do Reinforcement learning. It all hinges on the algorithm used for the training. \n",
    "\n",
    "- Do we know how to calculate the rewards? \n",
    "- Or the expected rewards for all possible actions? \n",
    "- Is it even possible?\n",
    "- What is the thing that learns? A genetic algorithm? A Neural Network? ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![banner](https://i.imgur.com/O5UU2no.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEHOLD AN ARTIFICIAL NEURON!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce3c33b1c762443f9da2828d6f45fe57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='Input', max=5.0, min=-5.0), FloatSlider(value=1.0, d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_neuron(input_value=1.0, weight=1.0, bias=0.0)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from ipywidgets import interact, FloatSlider\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_neuron(input_value=1.0, weight=1.0, bias=0.0):\n",
    "    # Compute output using the neuron formula\n",
    "    output = input_value * weight + bias\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.set_xlim(-1.5, 5.5)\n",
    "    ax.set_ylim(-2, 4.5)\n",
    "    ax.axis('off')\n",
    "    ax.set_aspect('equal', 'box') # Ensure circles are not squished\n",
    "\n",
    "    # Add formula text\n",
    "    formula_text = f\"Formula: output = (input × weight) + bias\\n\" \\\n",
    "                   f\"         = ({input_value:.1f} × {weight:.1f}) + {bias:.1f}\\n\" \\\n",
    "                   f\"         = {output:.2f}\"\n",
    "    ax.text(2.5, 4.2, formula_text, ha='center', va='top', fontsize=12, \n",
    "            bbox=dict(facecolor='white', alpha=0.9))\n",
    "\n",
    "    # Draw input node\n",
    "    ax.text(-1.2, 1.3, f\"Input\\n{input_value:0.2f}\", fontsize=12, ha=\"center\")\n",
    "    plt.plot([-1.4, -0.7], [1.0, 1.0], color='gray', lw=2, linestyle='--')\n",
    "    \n",
    "    # Draw neuron\n",
    "    circle = plt.Circle((2.0, 1.0), 1, color='skyblue', ec='k', zorder=2)\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(2.0, 1.0, \"Neuron\", fontsize=12, ha=\"center\", va=\"center\")\n",
    "    \n",
    "    # Draw bias\n",
    "    ax.annotate(\"\", xy=(2.0, 2), xytext=(2.0, 2.7),\n",
    "                arrowprops=dict(arrowstyle=\"->\", color=\"red\", lw=2))\n",
    "    ax.text(2.0, 2.8, f\"Bias: {bias:0.2f}\", color=\"red\", ha=\"center\", fontsize=12)\n",
    "    \n",
    "    # Draw weight\n",
    "    ax.annotate(\"\", xy=(1.0, 1.0), xytext=(-0.7, 1.0),\n",
    "                arrowprops=dict(arrowstyle=\"->\", color=\"blue\", lw=2))\n",
    "    ax.text(-0.1, 1.1, f\"Weight: {weight:0.2f}\", color=\"blue\", ha=\"center\", fontsize=12)\n",
    "    \n",
    "    # Draw output\n",
    "    ax.annotate(\"\", xy=(3, 1.0), xytext=(5.2, 1.0),\n",
    "                arrowprops=dict(arrowstyle=\"<-\", color=\"green\", lw=2))\n",
    "    ax.text(4.2, 1.1, f\"Output: {output:0.2f}\", color=\"green\", ha=\"center\", fontsize=12)\n",
    "    \n",
    "    ax.set_title(\"Single Neuron with Linear Activation\", fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "# Create interactive widget\n",
    "interact(plot_neuron,\n",
    "         input_value=FloatSlider(min=-5, max=5, step=0.1, value=1.0, description=\"Input\"),\n",
    "         weight=FloatSlider(min=-5, max=5, step=0.1, value=1.0, description=\"Weight\"),\n",
    "         bias=FloatSlider(min=-5, max=5, step=0.1, value=0.0, description=\"Bias\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A network\n",
    "\n",
    "Several of those neurons (billions in the case of modern AI systems) are put together in a network. Usually in layers that connect to each other, each neuron multiplying, adding and sending its output forward to more neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from ipywidgets import FloatSlider, VBox, HBox, interactive_output\n",
    "from IPython.display import display\n",
    "\n",
    "def plot_network(x1, x2, w1_00, w1_10, w1_01, w1_11, w2_0, w2_1):\n",
    "    # Compute the forward pass\n",
    "    h1 = x1 * w1_00 + x2 * w1_10\n",
    "    h2 = x1 * w1_01 + x2 * w1_11\n",
    "    output = h1 * w2_0 + h2 * w2_1\n",
    "\n",
    "    # Set up the figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.set_xlim(-1.5, 5)\n",
    "    ax.set_ylim(-1.5, 3)\n",
    "    ax.axis('off') \n",
    "    ax.set_aspect('equal', 'box') # Ensure circles are not squished\n",
    "\n",
    "    # Define positions for nodes in each layer\n",
    "    positions = {\n",
    "        \"x1\": (0, 1.5),\n",
    "        \"x2\": (0, 0.5),\n",
    "        \"h1\": (2, 1.5),\n",
    "        \"h2\": (2, 0.5),\n",
    "        \"output\": (4, 1)\n",
    "    }\n",
    "\n",
    "    # Function to draw each node: a circle, with the label and node value inside\n",
    "    def draw_node(pos, value, label, color):\n",
    "        circle = plt.Circle(pos, 0.2, color=color, ec='k', zorder=5)\n",
    "        ax.add_patch(circle)\n",
    "        ax.text(pos[0], pos[1], f\"{label}\\n{value:.2f}\", \n",
    "                ha='center', va='center', fontsize=10, zorder=6)\n",
    "\n",
    "    # Draw nodes for each layer\n",
    "    draw_node(positions[\"x1\"], x1, \"x₁\", 'lightyellow')\n",
    "    draw_node(positions[\"x2\"], x2, \"x₂\", 'lightyellow')\n",
    "    draw_node(positions[\"h1\"], h1, \"h₁\", 'skyblue')\n",
    "    draw_node(positions[\"h2\"], h2, \"h₂\", 'skyblue')\n",
    "    draw_node(positions[\"output\"], output, \"ŷ\", 'lightgreen')\n",
    "\n",
    "    # Draw layer labels above the nodes\n",
    "    ax.text(positions[\"x1\"][0], positions[\"x1\"][1] + 0.6, \"Input Layer\",\n",
    "            ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "    ax.text(positions[\"h1\"][0], positions[\"h1\"][1] + 0.6, \"Hidden Layer\",\n",
    "            ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "    ax.text(positions[\"output\"][0], positions[\"output\"][1] + 0.6, \"Output Layer\",\n",
    "            ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "    # Also add a clear summary of input and output values on the sides\n",
    "    ax.text(-1.3, 1, f\"Inputs:\\n x₁ = {x1:.2f}\\n x₂ = {x2:.2f}\",\n",
    "            fontsize=11, ha='center', va='center',\n",
    "            bbox=dict(facecolor='white', alpha=0.9, edgecolor='gray'))\n",
    "    ax.text(4.8, 1, f\"Output:\\n ŷ = {output:.2f}\",\n",
    "            fontsize=11, ha='center', va='center',\n",
    "            bbox=dict(facecolor='white', alpha=0.9, edgecolor='gray'))\n",
    "\n",
    "    # Define the connections with their labels. Each connection is a tuple:\n",
    "    # (start_node, end_node, current weight value, weight label)\n",
    "    connections = [\n",
    "        (\"x1\", \"h1\", w1_00, \"w₁₀₀\"),\n",
    "        (\"x2\", \"h1\", w1_10, \"w₁₁₀\"),\n",
    "        (\"x1\", \"h2\", w1_01, \"w₁₀₁\"),\n",
    "        (\"x2\", \"h2\", w1_11, \"w₁₁₁\"),\n",
    "        (\"h1\", \"output\", w2_0, \"w₂₀\"),\n",
    "        (\"h2\", \"output\", w2_1, \"w₂₁\"),\n",
    "    ]\n",
    "    \n",
    "    # Function to draw an arrow (connection) with the connection label and weight value\n",
    "    def draw_arrow(start, end, weight, wt_label):\n",
    "        start_pos = np.array(positions[start])\n",
    "        end_pos = np.array(positions[end])\n",
    "        vector = end_pos - start_pos\n",
    "        length = np.linalg.norm(vector)\n",
    "        direction = vector / length\n",
    "        \n",
    "        # Adjust start and end positions so the arrow doesn't overlap the node circles\n",
    "        start_adjust = start_pos + direction * 0.25\n",
    "        end_adjust = end_pos - direction * 0.25\n",
    "        \n",
    "        # Draw arrow between nodes\n",
    "        ax.annotate(\"\",\n",
    "                    xy=end_adjust,\n",
    "                    xytext=start_adjust,\n",
    "                    arrowprops=dict(arrowstyle=\"->\", color=\"gray\", lw=1.5),\n",
    "                    zorder=3)\n",
    "        # Place a label for the connection: show the weight variable and value\n",
    "        midpoint = (start_adjust + end_adjust) / 2.0\n",
    "        # Use a slight offset for clarity\n",
    "        offset = np.array([0.0, 0.15])\n",
    "        ax.text(midpoint[0] + offset[0], midpoint[1] + offset[1],\n",
    "                f\"{wt_label}\\n{weight:.2f}\", fontsize=9, color=\"red\",\n",
    "                ha='center', va='center', bbox=dict(facecolor='white', alpha=0.8, edgecolor='none'))\n",
    "\n",
    "    # Draw all connection arrows with labels\n",
    "    for start, end, weight, wt_label in connections:\n",
    "        draw_arrow(start, end, weight, wt_label)\n",
    "\n",
    "    # Place an explanation text block on the upper right, if desired\n",
    "    explanation_text = (\n",
    "        \"Feedforward Computation:\\n\"\n",
    "        \"1. Inputs x₁ and x₂ are each multiplied by their connection weights.\\n\"\n",
    "        \"2. Hidden neurons sum these weighted inputs (h₁, h₂).\\n\"\n",
    "        \"3. Hidden outputs are multiplied by output weights and summed to form ŷ.\"\n",
    "    )\n",
    "    ax.text(4.2, 2.7, explanation_text, fontsize=10,\n",
    "            bbox=dict(facecolor='white', edgecolor='gray', alpha=0.8),\n",
    "            ha='left', va='top')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "### Create Interactive Widgets ###\n",
    "\n",
    "# Input sliders for x1 and x2\n",
    "slider_x1 = FloatSlider(min=-2, max=2, step=0.1, value=1.0, description=\"x₁\")\n",
    "slider_x2 = FloatSlider(min=-2, max=2, step=0.1, value=1.0, description=\"x₂\")\n",
    "\n",
    "# Sliders for weights connecting inputs to the hidden layer\n",
    "slider_w1_00 = FloatSlider(min=-2, max=2, step=0.1, value=1.0, description=\"w₁₀₀\")\n",
    "slider_w1_10 = FloatSlider(min=-2, max=2, step=0.1, value=1.0, description=\"w₁₁₀\")\n",
    "slider_w1_01 = FloatSlider(min=-2, max=2, step=0.1, value=1.0, description=\"w₁₀₁\")\n",
    "slider_w1_11 = FloatSlider(min=-2, max=2, step=0.1, value=1.0, description=\"w₁₁₁\")\n",
    "\n",
    "# Sliders for weights connecting the hidden layer to the output\n",
    "slider_w2_0 = FloatSlider(min=-2, max=2, step=0.1, value=1.0, description=\"w₂₀\")\n",
    "slider_w2_1 = FloatSlider(min=-2, max=2, step=0.1, value=1.0, description=\"w₂₁\")\n",
    "\n",
    "# Organize the slider layout\n",
    "inputs_box = HBox([slider_x1, slider_x2])\n",
    "weights_input_hidden = HBox([slider_w1_00, slider_w1_10, slider_w1_01, slider_w1_11])\n",
    "weights_hidden_output = HBox([slider_w2_0, slider_w2_1])\n",
    "ui = VBox([inputs_box, weights_input_hidden, weights_hidden_output])\n",
    "\n",
    "# Set up the interactive output\n",
    "out = interactive_output(plot_network, {\n",
    "    \"x1\": slider_x1,\n",
    "    \"x2\": slider_x2,\n",
    "    \"w1_00\": slider_w1_00,\n",
    "    \"w1_10\": slider_w1_10,\n",
    "    \"w1_01\": slider_w1_01,\n",
    "    \"w1_11\": slider_w1_11,\n",
    "    \"w2_0\": slider_w2_0,\n",
    "    \"w2_1\": slider_w2_1,\n",
    "})\n",
    "\n",
    "# Display the interactive UI and plot\n",
    "display(ui, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is an Activation Function?\n",
    "\n",
    "An activation function determines whether a neuron in a neural network should be activated (\"fired\") or not, based on the input it receives.\n",
    "\n",
    "## Why are Activation Functions Important?\n",
    "\n",
    "### The Key Problem: Linear Limitations\n",
    "\n",
    "Without activation functions, neural networks can only perform linear operations (multiplying and adding). Here's why this is a problem:\n",
    "\n",
    "- **Linear operations can only create linear solutions**: No matter how many layers you stack, if each layer only does multiplication and addition, the entire network can only learn straight-line relationships between inputs and outputs.\n",
    "\n",
    "- **Real-world problems aren't linear**: Most interesting problems (image recognition, language understanding, etc.) involve complex, curved relationships that can't be solved with just straight lines.\n",
    "\n",
    "### How Activation Functions Solve This:\n",
    "\n",
    "Activation functions introduce \"bends\" into the system. When we add an activation function:\n",
    "\n",
    "1. The neuron can now respond differently to different input ranges\n",
    "2. When combined with other neurons, these \"bends\" allow the network to approximate any curved shape\n",
    "3. This enables the network to learn complex patterns that simple linear models cannot capture\n",
    "\n",
    "**Simple example**: Imagine trying to separate data points in an X-shape. A straight line can never separate these points correctly, but with activation functions creating \"bends,\" the network can learn the right boundary.\n",
    "\n",
    "## The ReLU Activation Function\n",
    "\n",
    "ReLU (Rectified Linear Unit) creates this crucial non-linearity in a very simple way:\n",
    "\n",
    "- For negative inputs → output is 0\n",
    "- For positive inputs → output is the same as the input\n",
    "\n",
    "This simple \"bend\" at zero is enough to allow neural networks to learn incredibly complex patterns when many neurons work together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LET'S PONG](https://i.imgur.com/Tl3V3NE.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training vs. Playing: Two Different Modes\n",
    "\n",
    "It's important to understand the two modes of our agent:\n",
    "\n",
    "### Training Mode\n",
    "- Agent chooses actions randomly at first, based on probabilities from the network\n",
    "- It records everything that happens (states, actions, rewards)\n",
    "- After each game, it updates its neural network to improve\n",
    "- This involves exploration (trying new things)\n",
    "\n",
    "### Playing Mode\n",
    "- Agent always chooses the action with highest probability\n",
    "- No more randomness or exploration\n",
    "- No more learning or updates\n",
    "- Just using what it has learned\n",
    "\n",
    "We spend most of our time in training mode, then switch to playing mode when the agent is ready.\n",
    "\n",
    "There is no \"one way\" to do reinforcement learning. We don't have time to go through all cases, but we will look at one that is particularly useful for games.\n",
    "\n",
    "## The REINFORCE Algorithm: Learning from Success and Failure\n",
    "\n",
    "Now let's understand how our agent actually learns. We're using an algorithm called REINFORCE, which we'll explain step-by-step:\n",
    "\n",
    "### Step 1: Play a Complete Game\n",
    "\n",
    "The agent plays a full game of Pong until it misses the ball (game over). During this game, we record:\n",
    "- Each state it observed\n",
    "- Each action it took\n",
    "- Each reward it received\n",
    "\n",
    "Let's say our agent played a game that lasted 50 moves before missing the ball. We now have 50 (state, action, reward) tuples stored in memory.\n",
    "\n",
    "### Step 2: Calculate the \"Returns\"\n",
    "\n",
    "We need to know which actions were actually good in the long run. This is tricky because sometimes an action might look good immediately but lead to failure later.\n",
    "\n",
    "To solve this, we calculate the \"return\" for each step - essentially the total future reward from that point onwards, with future rewards discounted (valued less than immediate rewards).\n",
    "\n",
    "For each step t, we calculate:\n",
    "$$\n",
    "\\text{Return}(t) = \\text{Reward}(t) + \\gamma \\cdot \\text{Reward}(t+1) + \\gamma^2 \\cdot \\text{Reward}(t+2) + \\cdots\n",
    "$$\n",
    "\n",
    "Where $\\gamma$ is a number between 0 and 1 that determines how much we care about future rewards.\n",
    "\n",
    "\n",
    "#### Example:\n",
    "If our rewards were [0, 0, 0, 1, 0, 0, -1] and gamma is 0.9:\n",
    "- Return at step 6 = -1\n",
    "- Return at step 5 = 0 + 0.9 * (-1) = -0.9\n",
    "- Return at step 4 = 0 + 0.9 * 0 + 0.9 * (0.9)* -1 = -0.81\n",
    "- Return at step 3 = 1 + 0.9 * (-0.81) = 0.271\n",
    "- ...and so on\n",
    "\n",
    "This gives us a better measure of how good each action really was.\n",
    "\n",
    "\n",
    "\n",
    "### Step 3: Update the Policy Network\n",
    "\n",
    "Now comes the crucial part - we need to adjust our neural network to make good actions more likely and bad actions less likely in the future.\n",
    "\n",
    "*ROUGHLY* For each (state, action, return) tuple:\n",
    "1. Feed the state into the network to get the current probabilities\n",
    "2. Increase the probability of the action taken if the return was positive\n",
    "3. Decrease the probability of the action taken if the return was negative\n",
    "\n",
    "#### How Weights Actually Change\n",
    "\n",
    "This is where we need to understand how neural networks learn:\n",
    "\n",
    "1. Each connection in our neural network has a \"weight\" - just a number that determines how strong that connection is.\n",
    "2. These weights determine the final probabilities output by the network.\n",
    "3. To make an action more likely, we need to adjust the weights that led to that action.\n",
    "\n",
    "Let's break this down with a simple example:\n",
    "\n",
    "Imagine our network gave these probabilities for a particular state:\n",
    "- UP: 30%\n",
    "- STAY: 50%\n",
    "- DOWN: 20%\n",
    "\n",
    "The agent selected STAY (based on these probabilities), and this eventually led to a positive return of 0.8.\n",
    "\n",
    "We want to adjust our network to make STAY even more likely in this situation next time. The math works out such that:\n",
    "- Weights that contributed to the STAY probability get increased\n",
    "- The larger the return (0.8 in this case), the larger the increase\n",
    "- Weights that didn't contribute to STAY don't change much\n",
    "\n",
    "\n",
    "The technical term for this process is \"gradient ascent on the policy parameters\" - but you can think of it as \"tweak the weights to make good actions more likely.\"\n",
    "\n",
    "\n",
    "### Step 4: Repeat\n",
    "This process is repeated for several episodes, iteratively updating the policy in the direction of higher rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's write the game engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# GAME CONSTANTS (these would typically be in a header file in C++)\n",
    "# -------------------------------------------------------------------------\n",
    "WIDTH = 400           # Game screen width in pixels\n",
    "HEIGHT = 250          # Game screen height in pixels\n",
    "PADDLE_HEIGHT = 70    # Paddle height in pixels\n",
    "PADDLE_WIDTH = 15     # Paddle width in pixels\n",
    "PADDLE_MOVE_SPEED = 5 # How fast the paddle moves when a key is pressed\n",
    "BALL_RADIUS = 7       # Ball radius in pixels\n",
    "BALL_SPEED = 5        # Base ball speed in pixels per frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import threading\n",
    "import numpy as np\n",
    "from ipycanvas import Canvas\n",
    "import ipywidgets as widgets\n",
    "from ipyevents import Event\n",
    "from IPython.display import display\n",
    "\n",
    "def pong_step(state, action):\n",
    "    \"\"\"\n",
    "    Update ball and right-paddle state.\n",
    "    \n",
    "    state: [ball_x, ball_y, paddle_y, ball_dx, ball_dy]\n",
    "    action (for right paddle): 0 = up, 1 = none, 2 = down.\n",
    "    \n",
    "    Returns a list of native Python floats.\n",
    "    \"\"\"\n",
    "    ball_x, ball_y, paddle_y, ball_dx, ball_dy = state\n",
    "\n",
    "    # Update AI paddle position (right paddle) based on action.\n",
    "    if action == 0:\n",
    "        paddle_y = max(0.0, paddle_y - PADDLE_MOVE_SPEED)\n",
    "    elif action == 2:\n",
    "        paddle_y = min(HEIGHT - PADDLE_HEIGHT, paddle_y + PADDLE_MOVE_SPEED)\n",
    "\n",
    "    # Update ball position.\n",
    "    ball_x += ball_dx\n",
    "    ball_y += ball_dy\n",
    "\n",
    "    # Bounce off top and bottom walls.\n",
    "    if ball_y - BALL_RADIUS < 0:\n",
    "        ball_y = BALL_RADIUS\n",
    "        ball_dy = abs(ball_dy)\n",
    "    elif ball_y + BALL_RADIUS > HEIGHT:\n",
    "        ball_y = HEIGHT - BALL_RADIUS\n",
    "        ball_dy = -abs(ball_dy)\n",
    "\n",
    "    # Handle collision with the right (AI) paddle.\n",
    "    if ball_x + BALL_RADIUS >= WIDTH - PADDLE_WIDTH:\n",
    "        # If ball hits paddle, bounce back.\n",
    "        if paddle_y <= ball_y <= (paddle_y + PADDLE_HEIGHT):\n",
    "            ball_x = WIDTH - PADDLE_WIDTH - BALL_RADIUS\n",
    "            ball_dx = -abs(ball_dx)\n",
    "        else:\n",
    "            # If the paddle missed, reset the ball and the right paddle.\n",
    "            ball_x = WIDTH / 2.0\n",
    "            ball_y = random.uniform(BALL_RADIUS, HEIGHT - BALL_RADIUS)\n",
    "            ball_dx = BALL_SPEED\n",
    "            ball_dy = BALL_SPEED\n",
    "            paddle_y = (HEIGHT - PADDLE_HEIGHT) / 2.0\n",
    "\n",
    "    # (Left wall collision handled in game loop)\n",
    "    return [float(ball_x), float(ball_y), float(paddle_y), float(ball_dx), float(ball_dy)]\n",
    "\n",
    "\n",
    "class PongGame:\n",
    "    def __init__(self, ai_function):\n",
    "        \"\"\"\n",
    "        Initialize the Pong game.\n",
    "        \n",
    "        ai_function(ball_x, ball_y, paddle_y, ball_dx, ball_dy)\n",
    "            should return one of: \"up\", \"none\", or \"down\" for the right paddle.\n",
    "        \"\"\"\n",
    "        # Left (player) paddle and ball state.\n",
    "        self.left_paddle_y = (HEIGHT - PADDLE_HEIGHT) / 2.0\n",
    "        self.right_paddle_y = (HEIGHT - PADDLE_HEIGHT) / 2.0\n",
    "        self.ball_x = WIDTH / 2.0\n",
    "        self.ball_y = random.uniform(BALL_RADIUS, HEIGHT - BALL_RADIUS)\n",
    "        self.ball_dx = BALL_SPEED\n",
    "        self.ball_dy = BALL_SPEED\n",
    "\n",
    "        self.ai_function = ai_function\n",
    "\n",
    "        # Movement flags for the left paddle.\n",
    "        self.left_up_active = False\n",
    "        self.left_down_active = False\n",
    "\n",
    "        self.running = False\n",
    "\n",
    "        self._create_widgets()\n",
    "\n",
    "    def _create_widgets(self):\n",
    "        # Create the game canvas.\n",
    "        self.canvas = Canvas(width=WIDTH, height=HEIGHT)\n",
    "        display(self.canvas)\n",
    "        \n",
    "        # Create control buttons.\n",
    "        self.btn_left_up = widgets.Button(\n",
    "            description=\"UP▲\", layout=widgets.Layout(width='100px'), button_style='info')\n",
    "        self.btn_left_down = widgets.Button(\n",
    "            description=\"▼DOWN\", layout=widgets.Layout(width='100px'), button_style='info')\n",
    "        self.btn_stop = widgets.Button(\n",
    "            description=\"STOP GAME\", layout=widgets.Layout(width='100px', height='40px'),\n",
    "            button_style='danger')\n",
    "        \n",
    "        # Set up ipyevents on the left paddle buttons for mousedown/up/leave.\n",
    "        event_up = Event(source=self.btn_left_up, watched_events=['mousedown', 'mouseup', 'mouseleave'])\n",
    "        event_up.on_dom_event(self._handle_left_up)\n",
    "        event_down = Event(source=self.btn_left_down, watched_events=['mousedown', 'mouseup', 'mouseleave'])\n",
    "        event_down.on_dom_event(self._handle_left_down)\n",
    "\n",
    "        # Stop button uses normal on_click.\n",
    "        self.btn_stop.on_click(self._stop_game)\n",
    "        \n",
    "        # Display control buttons.\n",
    "        controls = widgets.VBox([widgets.HBox([self.btn_left_up, self.btn_left_down]), self.btn_stop])\n",
    "        display(controls)\n",
    "    \n",
    "    def _handle_left_up(self, event):\n",
    "        # When the up button is pressed, set the flag; released/leave clears it.\n",
    "        if event['type'] == 'mousedown':\n",
    "            self.left_up_active = True\n",
    "        elif event['type'] in ['mouseup', 'mouseleave']:\n",
    "            self.left_up_active = False\n",
    "    \n",
    "    def _handle_left_down(self, event):\n",
    "        # When the down button is pressed, set the flag; released/leave clears it.\n",
    "        if event['type'] == 'mousedown':\n",
    "            self.left_down_active = True\n",
    "        elif event['type'] in ['mouseup', 'mouseleave']:\n",
    "            self.left_down_active = False\n",
    "    \n",
    "    def _draw(self):\n",
    "        # Draw ball first (helps with flicker)\n",
    "        self.canvas.fill_style = 'black'\n",
    "        self.canvas.fill_circle(self.ball_x, self.ball_y, BALL_RADIUS)\n",
    "\n",
    "        # Clear the canvas and redraw all elements in the correct order.\n",
    "        self.canvas.clear()\n",
    "        \n",
    "        # Draw background first\n",
    "        self.canvas.fill_style = 'white'\n",
    "        self.canvas.fill_rect(0, 0, WIDTH, HEIGHT)\n",
    "        \n",
    "        # Draw paddles\n",
    "        self.canvas.fill_style = 'blue'\n",
    "        self.canvas.fill_rect(0, self.left_paddle_y, PADDLE_WIDTH, PADDLE_HEIGHT)\n",
    "        \n",
    "        self.canvas.fill_style = 'red'\n",
    "        self.canvas.fill_rect(WIDTH - PADDLE_WIDTH, self.right_paddle_y, PADDLE_WIDTH, PADDLE_HEIGHT)\n",
    "        \n",
    "        # Draw ball last (on top of everything else)\n",
    "        self.canvas.fill_style = 'black'\n",
    "        self.canvas.fill_circle(self.ball_x, self.ball_y, BALL_RADIUS)\n",
    "    \n",
    "    def _reset_ball(self):\n",
    "        # Reset the ball to the center with a random vertical position.\n",
    "        self.ball_x = WIDTH / 2.0\n",
    "        self.ball_y = random.uniform(BALL_RADIUS, HEIGHT - BALL_RADIUS)\n",
    "        self.ball_dx = BALL_SPEED\n",
    "        self.ball_dy = BALL_SPEED\n",
    "        self.right_paddle_y = (HEIGHT - PADDLE_HEIGHT) / 2.0\n",
    "\n",
    "    def game_loop(self):\n",
    "        fps_delay = 1.0 / 30.0  # approximately 30 FPS\n",
    "        mapping = {\"up\": 0, \"none\": 1, \"down\": 2}\n",
    "        while self.running:\n",
    "            # Move the left paddle based on button flags.\n",
    "            if self.left_up_active:\n",
    "                self.left_paddle_y = max(0.0, self.left_paddle_y - PADDLE_MOVE_SPEED)\n",
    "            if self.left_down_active:\n",
    "                self.left_paddle_y = min(HEIGHT - PADDLE_HEIGHT, self.left_paddle_y + PADDLE_MOVE_SPEED)\n",
    "            \n",
    "            # Build the game state for the ball and right paddle.\n",
    "            state = [self.ball_x, self.ball_y, self.right_paddle_y, self.ball_dx, self.ball_dy]\n",
    "\n",
    "            # Get the AI action for the right paddle.\n",
    "            ai_action = self.ai_function(self.ball_x, self.ball_y, self.right_paddle_y, self.ball_dx, self.ball_dy)\n",
    "            action_int = mapping.get(ai_action, 1)\n",
    "\n",
    "            # Update ball position and the right paddle using pong_step.\n",
    "            new_state = pong_step(state, action_int)\n",
    "            self.ball_x, self.ball_y, self.right_paddle_y, self.ball_dx, self.ball_dy = new_state\n",
    "            \n",
    "            # Check collision with the left (player) paddle.\n",
    "            if self.ball_x - BALL_RADIUS <= PADDLE_WIDTH:\n",
    "                if self.left_paddle_y <= self.ball_y <= (self.left_paddle_y + PADDLE_HEIGHT):\n",
    "                    # Bounce the ball off the player's paddle.\n",
    "                    self.ball_x = PADDLE_WIDTH + BALL_RADIUS\n",
    "                    self.ball_dx = abs(self.ball_dx)\n",
    "                else:\n",
    "                    # The player missed: reset the ball.\n",
    "                    self._reset_ball()\n",
    "            \n",
    "            self._draw()\n",
    "            time.sleep(fps_delay)\n",
    "    \n",
    "    def start(self):\n",
    "        self.running = True\n",
    "        # Run the game loop in a separate thread to free the UI thread.\n",
    "        self.thread = threading.Thread(target=self.game_loop, daemon=True)\n",
    "        self.thread.start()\n",
    "    \n",
    "    def _stop_game(self, _):\n",
    "        self.running = False\n",
    "        self.btn_stop.description = \"Stopped\"\n",
    "        self.btn_stop.disabled = True\n",
    "        self.left_up_active = False\n",
    "        self.left_down_active = False\n",
    "\n",
    "def start_game(ai_function):\n",
    "    \"\"\"\n",
    "    Initialize and start the Pong game.\n",
    "    \n",
    "    Provide an ai_function(ball_x, ball_y, paddle_y, ball_dx, ball_dy)\n",
    "    that returns \"up\", \"none\", or \"down\" for controlling the right paddle.\n",
    "    \"\"\"\n",
    "    game = PongGame(ai_function)\n",
    "    game.start()\n",
    "    return game\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how we use it.\n",
    "\n",
    "# Some useful thigns for your to use in your implementation\n",
    "# paddle_center = paddle_y + PADDLE_HEIGHT/2 \n",
    "# ball_center = ball_y + BALL_RADIUS\n",
    "\n",
    "\n",
    "# --- Example AI Function ---\n",
    "def simple_ai(ball_x, ball_y, paddle_y, ball_dx, ball_dy):\n",
    "    \"\"\"\n",
    "    A basic AI: move the paddle up or down so that its center follows the ball.\n",
    "    It returns \"up\" if the paddle should move up, \"down\" if it should move down,\n",
    "    and \"none\" if it should stay still.\n",
    "    \"\"\"\n",
    "    #TASK 1: YOUR CODE HERE\n",
    "\n",
    "    # Move the paddle up if the ball is above the center of the paddle and the ball dx is positive\n",
    "    if ball_y < paddle_y + PADDLE_HEIGHT/2 and ball_dx > 0:\n",
    "        return \"up\"\n",
    "    # Move the paddle down if the ball is below the center of the paddle and the ball dx is positive\n",
    "    elif ball_y > paddle_y + PADDLE_HEIGHT/2 and ball_dx > 0:\n",
    "        return \"down\"\n",
    "\n",
    "# --- Start the Game ---\n",
    "# Pass the AI function you want to use.\n",
    "start_game(simple_ai)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Reinforcement Learning \n",
    "Now that we've set up our Pong game environment, we're ready to create and train an AI that can learn to play. We'll be using a reinforcement learning approach called REINFORCE (a type of policy gradient method).\n",
    "\n",
    "## What was Reinforcement Learning again?\n",
    "\n",
    "Reinforcement learning works by trial and error:\n",
    "- The agent (our AI) takes actions in the environment\n",
    "- It receives feedback in the form of rewards\n",
    "- It learns to take actions that maximize its total reward\n",
    "\n",
    "Think of it like training a dog: we don't tell it exactly how to catch a frisbee, but we reward it when it does, and over time it figures out the best strategy.\n",
    "\n",
    "## Our Implementation Plan\n",
    "\n",
    "Here's what we'll do next:\n",
    "\n",
    "1. **Build the Agent**: Create a class that:\n",
    "   - Contains a neural network (the \"brain\" of our AI)\n",
    "   - Can choose actions based on the game state\n",
    "   - Keeps track of its experiences (states, actions, rewards)\n",
    "   - Can learn from these experiences\n",
    "\n",
    "2. **Train the Agent**: Run many games where:\n",
    "   - The agent observes the state and chooses actions\n",
    "   - We record what happens (rewards received)\n",
    "   - After each game, the agent updates its neural network to improve\n",
    "\n",
    "3. **Use the Trained Agent**: Once training is complete, we can use our AI to play the game based on what it has learned.\n",
    "\n",
    "## Key Components\n",
    "\n",
    "Our implementation will include:\n",
    "\n",
    "- **Neural Network**: A simple model with one hidden layer that takes the game state as input and outputs probabilities for each action (up, stay, down)\n",
    "\n",
    "- **Action Selection**: During training, actions will be chosen probabilistically to encourage exploration. After training, the agent will choose the most likely action.\n",
    "\n",
    "- **REINFORCE Algorithm**: This is how our agent will learn. After each game:\n",
    "  - It calculates the cumulative rewards from each time step\n",
    "  - It adjusts its neural network to make actions that led to good outcomes more likely in the future\n",
    "\n",
    "- **Reward Shaping**: To help speed up learning, we'll provide small intermediate rewards for keeping the paddle near the ball.\n",
    "\n",
    "The code we're about to implement will transform our Pong environment into a learning playground for our AI. By the end of training, we should have an agent that can effectively track and hit the ball."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Pong Reinforcement Learning Tutorial\n",
    "# This file demonstrates how to create an AI for Pong using reinforcement learning\n",
    "# Intended for game design students with C++ background who are new to Python and ML\n",
    "\n",
    "import time\n",
    "import numpy as np              # NumPy handles arrays and math operations (like C++ vectors but more powerful)\n",
    "import tensorflow as tf         # TensorFlow is a machine learning library\n",
    "from tensorflow import keras    # Keras is a high-level neural network API\n",
    "from keras import layers        # Layers are the building blocks of neural networks\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# GAME PHYSICS AND REWARD SYSTEM\n",
    "# -------------------------------------------------------------------------\n",
    "def rl_step(state, action):\n",
    "    \"\"\"\n",
    "    Simulates one step of the Pong game physics and calculates rewards.\n",
    "    \n",
    "    This is similar to the Update() or Step() function you might have in a C++ game loop.\n",
    "    \n",
    "    Parameters:\n",
    "    - state: [ball_x, ball_y, paddle_y, ball_dx, ball_dy] - Current game state\n",
    "    - action: What the paddle should do (0 = move up, 1 = stay still, 2 = move down)\n",
    "    \n",
    "    Returns:\n",
    "    - new_state: Updated game state after this step\n",
    "    - reward: Positive or negative feedback based on the agent's performance\n",
    "    - done: Whether the game is over (ball passed the paddle)\n",
    "    \"\"\"\n",
    "    # Unpack the state values for readability - similar to struct access in C++\n",
    "    ball_x, ball_y, paddle_y, ball_dx, ball_dy = state\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1. UPDATE PADDLE POSITION BASED ON ACTION\n",
    "    # -------------------------------------------------------------------------\n",
    "    if action == 0:  # Move paddle up\n",
    "        paddle_y = max(0.0, paddle_y - PADDLE_MOVE_SPEED)  # Prevent going above the screen\n",
    "    elif action == 2:  # Move paddle down\n",
    "        paddle_y = min(HEIGHT - PADDLE_HEIGHT, paddle_y + PADDLE_MOVE_SPEED)  # Prevent going below the screen\n",
    "    # If action == 1, the paddle doesn't move (stays in place)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2. UPDATE BALL POSITION\n",
    "    # -------------------------------------------------------------------------\n",
    "    ball_x += ball_dx  # Move ball horizontally\n",
    "    ball_y += ball_dy  # Move ball vertically\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3. HANDLE BALL COLLISIONS WITH TOP AND BOTTOM WALLS\n",
    "    # -------------------------------------------------------------------------\n",
    "    if ball_y - BALL_RADIUS < 0:  # Ball hits top wall\n",
    "        ball_y = BALL_RADIUS  # Reposition to prevent getting stuck in wall\n",
    "        ball_dy = abs(ball_dy)  # Flip vertical direction to positive (downward)\n",
    "    \n",
    "    if ball_y + BALL_RADIUS > HEIGHT:  # Ball hits bottom wall\n",
    "        ball_y = HEIGHT - BALL_RADIUS  # Reposition to prevent getting stuck in wall\n",
    "        ball_dy = -abs(ball_dy)  # Flip vertical direction to negative (upward)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4. CALCULATE REWARD FOR THE AI\n",
    "    # -------------------------------------------------------------------------\n",
    "    # \"Shaping\" rewards guide the AI toward better behavior before it succeeds\n",
    "    # This is like giving hints rather than just win/lose feedback\n",
    "    \n",
    "    # Calculate how well the paddle is positioned relative to the ball\n",
    "    paddle_center = paddle_y + PADDLE_HEIGHT / 2.0\n",
    "    # This gives higher rewards when paddle is closer to ball's height\n",
    "    shaping_factor = 0.8\n",
    "    shaping_reward = (1 - abs(paddle_center - ball_y)/HEIGHT) * shaping_factor\n",
    "    \n",
    "    # Small penalty for not moving, to encourage active play\n",
    "    if action == 1:  # If the paddle didn't move\n",
    "        shaping_reward -= 0.1  # Apply small penalty\n",
    "        \n",
    "    # Start with the shaping reward\n",
    "    reward = shaping_reward \n",
    "    done = False  # Game continues by default\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5. HANDLE BALL COLLISION WITH RIGHT PADDLE (AI's paddle)\n",
    "    # -------------------------------------------------------------------------\n",
    "    if ball_x + BALL_RADIUS >= WIDTH - PADDLE_WIDTH:  # Ball reaches the right edge where paddle is\n",
    "        if paddle_y <= ball_y <= (paddle_y + PADDLE_HEIGHT):  # Ball hits paddle\n",
    "            # Successful hit!\n",
    "            reward = shaping_reward + 1.0  # Big reward for hitting the ball\n",
    "            # Push ball back a bit so it doesn't get stuck inside paddle\n",
    "            ball_x = WIDTH - PADDLE_WIDTH - BALL_RADIUS\n",
    "            # Reverse horizontal direction\n",
    "            ball_dx = -abs(ball_dx)\n",
    "        else:\n",
    "            # Ball missed the paddle - game over!\n",
    "            reward = shaping_reward - 1.0  # Penalty for missing\n",
    "            done = True  # End the game\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6. HANDLE BALL COLLISION WITH LEFT WALL (where an opponent would be)\n",
    "    # -------------------------------------------------------------------------\n",
    "    if ball_x - BALL_RADIUS <= 0:  # Ball hits left wall\n",
    "        ball_x = BALL_RADIUS  # Reposition\n",
    "        ball_dx = abs(ball_dx)  # Reverse direction to positive (rightward)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7. PREPARE AND RETURN THE NEW GAME STATE\n",
    "    # -------------------------------------------------------------------------\n",
    "    new_state = np.array([ball_x, ball_y, paddle_y, ball_dx, ball_dy], dtype=np.float32)\n",
    "    return new_state, reward, done\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Neural Network Weights Change in REINFORCE\n",
    "\n",
    "## Weights as Sensitivity Knobs\n",
    "Neural network weights act like **tunable dials** that determine how the agent interprets game states. These numbers evolve to prioritize actions that maximize rewards.\n",
    "\n",
    "---\n",
    "\n",
    "##  Neural Network Architecture\n",
    "| Component       | Description                                                                 |\n",
    "|-----------------|-----------------------------------------------------------------------------|\n",
    "| **Input Layer** | Receives game state (ball/paddle positions, velocities)                    |\n",
    "| **Weights**     | Numerical values controlling signal strength between neurons               |\n",
    "| **Output Layer**| Produces probability distribution over actions (UP/STAY/DOWN)              |\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Process: Step-by-Step\n",
    "1. **Forward Pass**  \n",
    "   - Process game steps through the network → get action probabilities  \n",
    "   - *Example Output:* UP (20%), STAY (30%), DOWN (50%)\n",
    "\n",
    "2. **Action Selection**  \n",
    "   - Randomly sample from probabilities (e.g., chooses DOWN)\n",
    "\n",
    "3. **Reward Calculation**  \n",
    "   - Compute discounted return for trajectory segment  \n",
    "   - *Example Return:* +0.7 (accounts for future rewards)\n",
    "\n",
    "4. **Backpropagation**  \n",
    "\n",
    "   The gradient is a mathematical concept that tells us the direction and magnitude of the steepest increase of a function. In neural networks, it's essentially a collection of partial derivatives that indicate how a small change in each weight would affect the output.\n",
    "\n",
    "   Key Points About Gradients:\n",
    "\n",
    "   - **Derivative Connection:** The gradient is built from partial derivatives – these measure how much the network's output changes when you slightly adjust a specific weight, while keeping all other weights constant.\n",
    "   - **Direction of Improvement:** When maximizing rewards, the gradient points in the direction where weights should change to increase the probability of beneficial actions.\n",
    "   - **Visualization:** Think of the gradient as a compass pointing \"uphill\" on a landscape where elevation represents better performance. The steeper the hill, the larger the gradient magnitude.\n",
    "   \n",
    "   ![gradient illustration](https://ds100.org/course-notes/feature_engineering/images/loss_surface.png)\n",
    "   image from:https://ds100.org/course-notes/feature_engineering/feature_engineering.html\n",
    "\n",
    "   When the REINFORCE algorithm multiplies this gradient by the return value, it strengthens connections that led to good outcomes (positive returns) and weakens those that led to poor ones (negative returns), proportional to how much each weight influenced the chosen action.\n",
    "\n",
    "   ```python\n",
    "   # Pseudocode for weight update logic\n",
    "   for weight in network:\n",
    "       if weight encouraged chosen_action (DOWN):\n",
    "           weight += learning_rate * return * gradient\n",
    "       else:\n",
    "           weight -= learning_rate * return * gradient\n",
    "   ```\n",
    "# REINFORCE Algorithm: Formula and Code Implementation\n",
    "\n",
    "## Standard Expression vs. Code Implementation\n",
    "\n",
    "The standard REINFORCE formula is typically written for a single timestep:\n",
    "\n",
    "$$\\Large \\theta_{t+1} = \\theta_t + \\alpha \\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t) G_t$$\n",
    "\n",
    "However, in practical implementation, we update across an entire episode of multiple timesteps at once.\n",
    "\n",
    "## Episode-Based Approach \n",
    "\n",
    "That means that we just average:\n",
    "\n",
    "$$\\Large \\theta_{new} = \\theta_{old} + \\alpha \\nabla_\\theta \\left( \\frac{1}{T} \\sum\\limits_{t=0}^{T-1} \\log \\pi_\\theta(a_t \\mid s_t) G_t \\right)$$\n",
    "\n",
    "Or equivalently, written as minimizing a loss function:\n",
    "\n",
    "$$\\Large \\theta_{new} = \\theta_{old} - \\alpha \\nabla_\\theta \\left( -\\frac{1}{T} \\sum\\limits_{t=0}^{T-1} \\log \\pi_\\theta(a_t \\mid s_t) G_t \\right)$$\n",
    "\n",
    "Where:\n",
    "- $T$ is the number of timesteps in the episode\n",
    "- $\\frac{1}{T} \\sum_{t=0}^{T-1}$ represents the averaging operation (implemented as `reduce_mean`)\n",
    "- The negative sign in the second formula corresponds to the negative in `loss = -tf.reduce_mean(weighted_log_pi)`\n",
    "\n",
    "\n",
    "## Benefits of Episode-Based Updates\n",
    "\n",
    "The averaging across timesteps helps stabilize training by reducing the variance in policy updates. Rather than making large updates based on individual timesteps, the policy is updated based on the average performance across the entire episode.\n",
    "\n",
    "This approach better captures what's actually happening in the code: a batch update using the average gradient across all timesteps in the episode, rather than separate updates for each individual timestep.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Weight Change Visualization\n",
    "**Scenario:** Successful DOWN action (return = +0.7)  \n",
    "| Weight Connection           | Initial | Updated | Change Direction |\n",
    "|-----------------------------|---------|---------|-------------------|\n",
    "| Ball_Y < 0.5 → DOWN         | 0.50    | 0.56    | ↑ Reinforcement   |\n",
    "| Paddle_Y_diff > 0 → UP      | 0.30    | 0.26    | ↓ Penalization    |\n",
    "| Ball_X_velocity ← STAY      | -0.15   | -0.21   | ↓ Penalization    |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "##  Long-Term Evolution\n",
    "| Training Stage | Weight Behavior                     | Agent Performance               |\n",
    "|----------------|-------------------------------------|---------------------------------|\n",
    "| Early          | Large random fluctuations           | Frequent misses                |\n",
    "| Mid            | Pattern-specific boosting           | Consistent returns             |\n",
    "| Late           | Fine-tuned precision adjustments    | Strategic positioning          |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===========================================================================\n",
    "# REINFORCEMENT LEARNING AGENT\n",
    "# ===========================================================================\n",
    "# This is the \"brain\" of our AI paddle that learns to play pong\n",
    "class RLAgent:\n",
    "    def __init__(self, learning_rate=5e-3, gamma=0.76):\n",
    "        \"\"\"\n",
    "        Initialize the AI agent.\n",
    "        \n",
    "        Parameters:\n",
    "        - learning_rate: How quickly the model adapts to new information (like step size)\n",
    "        - gamma: Discount factor - how much future rewards matter compared to immediate ones\n",
    "        \"\"\"\n",
    "        self.gamma = gamma  # Store the discount factor for future rewards\n",
    "        \n",
    "        # -------------------------------------------------------------------------\n",
    "        # CREATE THE NEURAL NETWORK MODEL\n",
    "        # -------------------------------------------------------------------------\n",
    "        # This is similar to creating a class with methods in C++, but using a\n",
    "        # pre-built system for machine learning\n",
    "        self.model = keras.Sequential([\n",
    "            # Input layer takes 5 values (the game state)\n",
    "            layers.Input(shape=(5,)),\n",
    "            # Hidden layer with 8 neurons and ReLU activation\n",
    "            # ReLU simply means \"if value < 0, output 0, else output the value\"\n",
    "            layers.Dense(8, activation='relu'),\n",
    "            # Output layer with 3 neurons (one for each possible action)\n",
    "            # Softmax makes the outputs into probabilities that sum to 1\n",
    "            layers.Dense(3, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        # Initialize the optimizer which adjusts the neural network\n",
    "        # Think of this as the \"learning algorithm\"\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "        \n",
    "        # -------------------------------------------------------------------------\n",
    "        # SAVING BUFFERS\n",
    "        # -------------------------------------------------------------------------\n",
    "        # These store the agent's experiences to learn from\n",
    "        # Like recording gameplay to study later\n",
    "        self.states = []    # Game states we've seen\n",
    "        self.actions = []   # Actions we took\n",
    "        self.rewards = []   # Rewards we received\n",
    "    \n",
    "    def _normalize_state(self, state):\n",
    "        \"\"\"\n",
    "        Scale the state values to a range between 0 and 1.\n",
    "        \n",
    "        This helps the neural network learn more efficiently,\n",
    "        similar to how you'd normalize a 3D model's coordinates.\n",
    "        \"\"\"\n",
    "        return np.array([\n",
    "            state[0] / WIDTH,         # x position relative to screen width\n",
    "            state[1] / HEIGHT,        # y position relative to screen height\n",
    "            state[2] / HEIGHT,        # paddle position relative to screen height\n",
    "            state[3] / BALL_SPEED,    # x velocity relative to maximum\n",
    "            state[4] / BALL_SPEED,    # y velocity relative to maximum\n",
    "        ], dtype=np.float32)\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        \"\"\"\n",
    "        Decide what action to take based on the current game state.\n",
    "        \n",
    "        This is like the AI's \"think\" function that runs every frame.\n",
    "        \n",
    "        Parameters:\n",
    "        - state: Current game state [ball_x, ball_y, paddle_y, ball_dx, ball_dy]\n",
    "        \n",
    "        Returns:\n",
    "        - action: 0 (move up), 1 (stay), or 2 (move down)\n",
    "        \"\"\"\n",
    "        # Normalize the state values to help the neural network\n",
    "        # Normalization is like scaling values to a common range, for vectors it is making their length 1\n",
    "        norm_state = self._normalize_state(state).reshape(1, -1)\n",
    "        \n",
    "        # Ask the neural network what to do\n",
    "        # It returns probabilities for each possible action\n",
    "        probs = self.model(norm_state).numpy().flatten()\n",
    "        \n",
    "        # Choose an action based on the probabilities\n",
    "        # This adds randomness for exploration (trying new strategies)\n",
    "        action = np.random.choice(3, p=probs)\n",
    "        \n",
    "        # Remember what we saw and what we did for learning later\n",
    "        self.states.append(norm_state)\n",
    "        self.actions.append(action)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def store_reward(self, reward):\n",
    "        \"\"\"\n",
    "        Store the reward received after taking an action.\n",
    "        \n",
    "        Parameters:\n",
    "        - reward: The feedback value received from the environment\n",
    "        \"\"\"\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def finish_episode(self):\n",
    "        \"\"\"\n",
    "        Perform the REINFORCE update on the policy network.\n",
    "\n",
    "        The update rule is:\n",
    "            θₜ₊₁ = θₜ + α · ∇θ log π₍θ₎(aₜ | sₜ) · Gₜ\n",
    "            \n",
    "        This function implements each step explicitly.\n",
    "        \"\"\"\n",
    "\n",
    "        # -------------------------------------------------------------------------\n",
    "        # 1. COMPUTE THE DISCOUNTED RETURNS (Gₜ)\n",
    "        # -------------------------------------------------------------------------\n",
    "        # For each timestep t, compute the return G_t = r_t + γ * r_{t+1} + γ² * r_{t+2} + ...\n",
    "        G_t = np.zeros_like(self.rewards, dtype=np.float32)  # Gₜ\n",
    "        cumulative_return = 0.0\n",
    "        for t in reversed(range(len(self.rewards))):\n",
    "            cumulative_return = self.rewards[t] + self.gamma * cumulative_return  # Gₜ = rₜ + γ · Gₜ₊₁\n",
    "            G_t[t] = cumulative_return\n",
    "\n",
    "        # Optionally normalize returns for more stable learning\n",
    "        baseline = np.mean(G_t)\n",
    "        G_t = G_t - baseline  # Normalized Gₜ\n",
    "\n",
    "        # -------------------------------------------------------------------------\n",
    "        # 2. PREPARE DATA: STATES (sₜ), ACTIONS (aₜ), RETURN (Gₜ)\n",
    "        # -------------------------------------------------------------------------\n",
    "        states = np.concatenate(self.states, axis=0)  # States: sₜ\n",
    "        actions = np.array(self.actions)              # Actions: aₜ\n",
    "        returns = G_t                                 # Returns: Gₜ\n",
    "\n",
    "        # -------------------------------------------------------------------------\n",
    "        # 3. COMPUTE THE POLICY OBJECTIVE AND GRADIENT (∇θ log π₍θ₎(aₜ|sₜ) · Gₜ)\n",
    "        # -------------------------------------------------------------------------\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass: Compute the action probabilities π₍θ₎(a | s) for all states.\n",
    "            action_probs = self.model(states, training=True)  # π₍θ₎(·|s)\n",
    "\n",
    "            # Create a one-hot vector for actions, so we can select the probability of the executed action.\n",
    "            one_hot_actions = tf.one_hot(actions, depth=3)  # Assume 3 actions. This is our mask.\n",
    "\n",
    "            # Select the probability for the taken action: π₍θ₎(aₜ|sₜ)\n",
    "            prob_taken = tf.reduce_sum(action_probs * one_hot_actions, axis=1)\n",
    "\n",
    "            # Compute log probability: log π₍θ₎(aₜ|sₜ)\n",
    "            log_pi = tf.math.log(prob_taken + 1e-8)\n",
    "\n",
    "            # Multiply by the return Gₜ: The term inside the gradient is log π₍θ₎(aₜ|sₜ) * Gₜ\n",
    "            weighted_log_pi = log_pi * returns\n",
    "\n",
    "            # Our objective (to be maximized) is the average policy \"score\":\n",
    "            #   Objective = E[log π₍θ₎(aₜ|sₜ) * Gₜ]\n",
    "            # We minimize the negative of this objective:\n",
    "            loss = -tf.reduce_mean(weighted_log_pi)\n",
    "\n",
    "        # -------------------------------------------------------------------------\n",
    "        # 4. COMPUTE GRADIENTS AND UPDATE THE MODEL PARAMETERS\n",
    "        # -------------------------------------------------------------------------\n",
    "        # Compute the gradient: ∇θ [ - (log π₍θ₎(aₜ | sₜ) * Gₜ) ]\n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "\n",
    "        # The optimizer updates the parameters using the learning rate (α) set during its initialization.\n",
    "        # This implements: θₜ₊₁ = θₜ + α · ∇θ log π₍θ₎(aₜ|sₜ) · Gₜ\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "\n",
    "        # -------------------------------------------------------------------------\n",
    "        # 5. RESET EPISODE MEMORY FOR THE NEXT EPISODE\n",
    "        # -------------------------------------------------------------------------\n",
    "        self.states, self.actions, self.rewards = [], [], []\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Choose the best action without randomness (for actual gameplay).\n",
    "        \n",
    "        This is used after training when we want the AI to play its best.\n",
    "        \n",
    "        Parameters:\n",
    "        - state: Current game state\n",
    "        \n",
    "        Returns:\n",
    "        - Best action (0, 1, or 2)\n",
    "        \"\"\"\n",
    "        norm_state = self._normalize_state(state).reshape(1, -1)\n",
    "        probs = self.model(norm_state).numpy().flatten()\n",
    "        return np.argmax(probs)  # Choose the action with highest probability\n",
    "\n",
    "\n",
    "# ===========================================================================\n",
    "# TRAINING LOOP\n",
    "# ===========================================================================\n",
    "def train_agent(num_episodes=1000):\n",
    "    \"\"\"\n",
    "    Train the agent by playing many games and learning from them.\n",
    "    \n",
    "    Parameters:\n",
    "    - num_episodes: Number of games to play for training\n",
    "    \n",
    "    Returns:\n",
    "    - trained_agent: The agent after training\n",
    "    \"\"\"\n",
    "    # Create a new agent\n",
    "    agent = RLAgent()\n",
    "    total_rewards = []  # Track rewards for analysis\n",
    "\n",
    "    max_steps_reached = 0  # Track the longest game\n",
    "    \n",
    "    # Play multiple games to train\n",
    "    for i in range(num_episodes):\n",
    "        # -------------------------------------------------------------------------\n",
    "        # 1. SET UP A NEW GAME WITH RANDOM STARTING CONDITIONS\n",
    "        # -------------------------------------------------------------------------\n",
    "        # Randomize the ball and paddle positions for varied training\n",
    "        ball_y_random = np.random.uniform(BALL_RADIUS, HEIGHT - BALL_RADIUS)\n",
    "        paddle_y_random = np.random.uniform(0, HEIGHT - PADDLE_HEIGHT)\n",
    "        \n",
    "        # Initialize the game state\n",
    "        state = np.array([\n",
    "            WIDTH / 2.0,        # Ball starts in the middle horizontally\n",
    "            ball_y_random,      # Random vertical position\n",
    "            paddle_y_random,    # Random paddle position\n",
    "            BALL_SPEED,         # Ball initially moves right\n",
    "            BALL_SPEED          # Ball initially moves down\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        # -------------------------------------------------------------------------\n",
    "        # 2. PLAY THE GAME UNTIL COMPLETION OR MAX STEPS\n",
    "        # -------------------------------------------------------------------------\n",
    "        episode_reward = 0.0  # Total reward for this game\n",
    "        done = False          # Game not finished yet\n",
    "        step = 0              # Step counter\n",
    "        \n",
    "        max_steps = 500  # Maximum steps per game (to prevent infinite games)\n",
    "        \n",
    "        # Game loop - similar to your C++ game loop\n",
    "        while not done and step < max_steps:\n",
    "            # AI chooses an action\n",
    "            action = agent.choose_action(state)\n",
    "            \n",
    "            # Update the game state based on the action\n",
    "            state, reward, done = rl_step(state, action)\n",
    "            \n",
    "            # Store the reward for learning\n",
    "            agent.store_reward(reward)\n",
    "            \n",
    "            # Keep track of total reward\n",
    "            episode_reward += reward\n",
    "            \n",
    "            # Increment step counter\n",
    "            step += 1\n",
    "        \n",
    "        # -------------------------------------------------------------------------\n",
    "        # 3. LEARN FROM THIS GAME\n",
    "        # -------------------------------------------------------------------------\n",
    "        agent.finish_episode()\n",
    "        \n",
    "        # Store results for analysis\n",
    "        total_rewards.append(episode_reward)\n",
    "        max_steps_reached = max(max_steps_reached, step)\n",
    "       \n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Episode {i+1}/{num_episodes}: Steps= {step}, Total Reward= {episode_reward:.2f}, Max Steps reached= {max_steps_reached}\")\n",
    "            max_steps_reached = 0\n",
    "    \n",
    "    return agent\n",
    "\n",
    "# Train the agent.\n",
    "trained_agent = train_agent(num_episodes=500)\n",
    "\n",
    "# Wrap the trained agent into an AI function for gameplay.\n",
    "def trained_ai_function(ball_x, ball_y, paddle_y, ball_dx, ball_dy):\n",
    "    state = np.array([ball_x, ball_y, paddle_y, ball_dx, ball_dy], dtype=np.float32)\n",
    "    action_idx = trained_agent.get_action(state)\n",
    "    mapping = {0: \"up\", 1: \"none\", 2: \"down\"}\n",
    "    return mapping[action_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained agent.\n",
    "#trained_agent.model.save(\"trained_pong_agent.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read an agent I trained with 5000 episodes into the RL class, took 20 minutes.\n",
    "trained_agent = RLAgent()\n",
    "trained_agent.model = keras.models.load_model(\"trained_pong_agent.h5\")\n",
    "\n",
    "# Wrap the trained agent into an AI function for gameplay.\n",
    "def trained_ai_function(ball_x, ball_y, paddle_y, ball_dx, ball_dy):\n",
    "    state = np.array([ball_x, ball_y, paddle_y, ball_dx, ball_dy], dtype=np.float32)\n",
    "    action_idx = trained_agent.get_action(state)\n",
    "    mapping = {0: \"up\", 1: \"none\", 2: \"down\"}\n",
    "    return mapping[action_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_game(trained_ai_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown Run to visualize the full trained network\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interactive, HBox, VBox\n",
    "from IPython.display import display\n",
    "\n",
    "# --- Ensure that your trained model is built ---\n",
    "# (This dummy call forces the model’s graph to be built.)\n",
    "_dummy = np.zeros((1, 5), dtype=np.float32)\n",
    "_ = trained_agent.model(_dummy)\n",
    "\n",
    "# --- Determine the hidden dense layer ---\n",
    "# Depending on your Keras version the explicit Input layer might not be in model.layers.\n",
    "# In our RLAgent model, if the [Input, Dense, Dense] remains then:\n",
    "#    model.layers[0] is the InputLayer and model.layers[1] is Dense(8)\n",
    "# but in Keras 3 the InputLayer is often omitted in model.layers.\n",
    "#\n",
    "# Check the number of layers and adjust accordingly:\n",
    "if len(trained_agent.model.layers) == 2:\n",
    "    # Only the Dense layers are present.\n",
    "    hidden_layer = trained_agent.model.layers[0]  # Dense(8)\n",
    "elif len(trained_agent.model.layers) >= 3:\n",
    "    # If the Input layer is included.\n",
    "    hidden_layer = trained_agent.model.layers[1]  # Dense(8)\n",
    "else:\n",
    "    hidden_layer = trained_agent.model.layers[0]  # Fallback\n",
    "\n",
    "print(\"Extracting hidden layer:\", hidden_layer.name)\n",
    "\n",
    "def visualize_trained_network(ball_x, ball_y, paddle_y, ball_dx, ball_dy):\n",
    "    # Retrieve network weights.\n",
    "    # Assumed order: [kernel_hidden, bias_hidden, kernel_output, bias_output]\n",
    "    weights = trained_agent.model.get_weights()\n",
    "    w1, b1 = weights[0], weights[1]\n",
    "    final_w, final_b = weights[2], weights[3]\n",
    "\n",
    "    # --- Build a sub-model to get hidden activations ---\n",
    "    # Instead of using trained_agent.model.input (which may not be defined),\n",
    "    # we create a new input tensor and pass it to our extracted hidden layer.\n",
    "    input_tensor = keras.Input(shape=(5,))\n",
    "    hidden_output = hidden_layer(input_tensor)\n",
    "    hidden_model = keras.Model(inputs=input_tensor, outputs=hidden_output)\n",
    "\n",
    "    # Create the figure.\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    ax.set_xlim(-1, 7)\n",
    "    ax.set_ylim(-1, 5)\n",
    "    ax.axis('off')\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    # Define node sizes.\n",
    "    node_radius_input = 0.2\n",
    "    node_radius_hidden = 0.15   # hidden nodes are drawn a bit smaller.\n",
    "    node_radius_output = 0.2\n",
    "\n",
    "    # Get the number of hidden neurons.\n",
    "    num_hidden = hidden_model.output_shape[-1]\n",
    "\n",
    "    # Define fixed positions for nodes.\n",
    "    layer_positions = {\n",
    "        \"input\": [(0, 4), (0, 3), (0, 2), (0, 1), (0, 0)],  # five inputs\n",
    "        \"hidden\": [(3, i * (4/(num_hidden-1))) for i in range(num_hidden)],\n",
    "        \"output\": [(6, 2), (6, 1), (6, 0)]  # three outputs\n",
    "    }\n",
    "\n",
    "    # Build the normalized state from current slider values.\n",
    "    state = np.array([ball_x, ball_y, paddle_y, ball_dx, ball_dy], dtype=np.float32)\n",
    "    norm_state = trained_agent._normalize_state(state).reshape(1, -1)\n",
    "\n",
    "    # Get full network prediction.\n",
    "    probs = trained_agent.model(norm_state, training=False).numpy().flatten()\n",
    "\n",
    "    # Compute hidden layer activations.\n",
    "    hidden_activations = hidden_model(norm_state, training=False).numpy().flatten()\n",
    "    max_act = hidden_activations.max() if hidden_activations.max() > 0 else 1.0\n",
    "    norm_activations = hidden_activations / max_act  # Normalize to [0, 1]\n",
    "\n",
    "    # Draw input nodes.\n",
    "    for pos in layer_positions['input']:\n",
    "        circle = plt.Circle(pos, node_radius_input, color='lightyellow', ec='k', zorder=5)\n",
    "        ax.add_patch(circle)\n",
    "\n",
    "    # Draw hidden nodes using a blue colormap based on activation.\n",
    "    cmap = plt.get_cmap(\"Blues\")\n",
    "    for i, pos in enumerate(layer_positions['hidden']):\n",
    "        activation = norm_activations[i]\n",
    "        face_color = cmap(0.3 + 0.7 * activation)  # shift so that even low activations are visible.\n",
    "        circle = plt.Circle(pos, node_radius_hidden, color=face_color, ec='k', zorder=5)\n",
    "        ax.add_patch(circle)\n",
    "        # Optionally, display raw activation value.\n",
    "        ax.text(pos[0], pos[1], f\"{hidden_activations[i]:.2f}\",\n",
    "                fontsize=7, ha='center', va='center', zorder=6)\n",
    "\n",
    "    # Draw output nodes.\n",
    "    for pos in layer_positions['output']:\n",
    "        circle = plt.Circle(pos, node_radius_output, color='lightgreen', ec='k', zorder=5)\n",
    "        ax.add_patch(circle)\n",
    "\n",
    "    # Normalize connection line alpha by maximum absolute weight.\n",
    "    max_weight = max(np.abs(w1).max(), np.abs(final_w).max())\n",
    "\n",
    "    # Draw connections from input to hidden using w1.\n",
    "    for i, start_pos in enumerate(layer_positions['input']):\n",
    "        for j, end_pos in enumerate(layer_positions['hidden']):\n",
    "            weight = w1[i, j]\n",
    "            color = 'red' if weight < 0 else 'blue'\n",
    "            alpha = np.abs(weight) / max_weight\n",
    "            ax.plot([start_pos[0] + node_radius_input, end_pos[0] - node_radius_hidden],\n",
    "                    [start_pos[1], end_pos[1]], color=color, alpha=alpha, lw=1)\n",
    "\n",
    "    # Draw connections from hidden to output using final_w.\n",
    "    for j, start_pos in enumerate(layer_positions['hidden']):\n",
    "        for k, end_pos in enumerate(layer_positions['output']):\n",
    "            weight = final_w[j, k]\n",
    "            color = 'red' if weight < 0 else 'blue'\n",
    "            alpha = np.abs(weight) / max_weight\n",
    "            ax.plot([start_pos[0] + node_radius_hidden, end_pos[0] - node_radius_output],\n",
    "                    [start_pos[1], end_pos[1]], color=color, alpha=alpha, lw=1)\n",
    "\n",
    "    # Label the layers.\n",
    "    ax.text(0, 4.5, \"Input Layer\\n(Ball X, Ball Y,\\nPaddle Y,\\nBall DX, Ball DY)\",\n",
    "            ha='center', va='bottom', fontsize=10)\n",
    "    ax.text(3, 4.5, f\"Hidden Layer\\n({num_hidden} Neurons)\",\n",
    "            ha='center', va='bottom', fontsize=10)\n",
    "    ax.text(6, 4.5, \"Output Layer\\n(Up, Stay, Down)\",\n",
    "            ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    # Display network prediction probabilities.\n",
    "    pred_text = (f\"Network Prediction:\\n\"\n",
    "                 f\"  Up: {probs[0]*100:.1f}%\\n\"\n",
    "                 f\"  Stay: {probs[1]*100:.1f}%\\n\"\n",
    "                 f\"  Down: {probs[2]*100:.1f}%\")\n",
    "    ax.text(6, -0.5, pred_text, ha='center', va='top',\n",
    "            bbox=dict(facecolor='white', alpha=0.9), fontsize=12)\n",
    "\n",
    "    plt.title(\"Network Architecture and Hidden Neuron Activations\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- Create slider widgets (ensure that WIDTH, HEIGHT, BALL_SPEED, PADDLE_HEIGHT are defined) ---\n",
    "slider_ball_x = widgets.FloatSlider(min=0, max=WIDTH, value=WIDTH/2, description=\"Ball X\",\n",
    "                                    layout=widgets.Layout(width='300px'))\n",
    "slider_ball_y = widgets.FloatSlider(min=0, max=HEIGHT, value=HEIGHT/2, description=\"Ball Y\",\n",
    "                                    layout=widgets.Layout(width='300px'))\n",
    "slider_paddle_y = widgets.FloatSlider(min=0, max=HEIGHT-PADDLE_HEIGHT, value=160, description=\"Paddle Y\",\n",
    "                                      layout=widgets.Layout(width='300px'))\n",
    "slider_ball_dx = widgets.FloatSlider(min=-BALL_SPEED, max=BALL_SPEED, value=BALL_SPEED,\n",
    "                                     description=\"Ball DX\", layout=widgets.Layout(width='300px'))\n",
    "slider_ball_dy = widgets.FloatSlider(min=-BALL_SPEED, max=BALL_SPEED, value=BALL_SPEED,\n",
    "                                     description=\"Ball DY\", layout=widgets.Layout(width='300px'))\n",
    "\n",
    "sliders_box = VBox([slider_ball_x, slider_ball_y, slider_paddle_y, slider_ball_dx, slider_ball_dy])\n",
    "\n",
    "# --- Create the interactive widget ---\n",
    "interactive_plot = interactive(visualize_trained_network,\n",
    "                               ball_x=slider_ball_x,\n",
    "                               ball_y=slider_ball_y,\n",
    "                               paddle_y=slider_paddle_y,\n",
    "                               ball_dx=slider_ball_dx,\n",
    "                               ball_dy=slider_ball_dy)\n",
    "\n",
    "display(HBox([sliders_box, interactive_plot.children[-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task II: Can you think of a different reward function/mechanism?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources:\n",
    "\n",
    "### Backpropagation, step-by-step | DL3, 3Blue1Brown\n",
    "https://www.youtube.com/watch?v=Ilg3gGewQ5U\n",
    "\n",
    "### MIT 6.S191 (2024): Reinforcement Learning, Alexander Amini\n",
    "https://www.youtube.com/watch?v=8JVRbHAVCws&t=1504s\n",
    "\n",
    "### RLlib: Industry-Grade, Scalable Reinforcement Learning\n",
    "https://docs.ray.io/en/latest/rllib/index.html\n",
    "\n",
    "### Tensorflow Playground, beautiful interactive tool to understand Neural Networks\n",
    "https://playground.tensorflow.org/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workhorse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
