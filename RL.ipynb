{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Reinforcement Learning for Game AI\n",
    "\n",
    "## What Is Reinforcement Learning?\n",
    "\n",
    "Imagine teaching someone to play a video game without being able to tell them the rules. You can only give them a thumbs up when they do something good and a thumbs down when they do something bad. Over time, they'd figure out what works and what doesn't through trial and error.\n",
    "\n",
    "That's essentially what reinforcement learning (RL) is - a way for AI to learn by interacting with an environment and receiving feedback.\n",
    "\n",
    "### Today we will use Pong as a case study\n",
    "\n",
    "In our case, we're teaching an AI to play Pong by letting it:\n",
    "- Try different paddle movements\n",
    "- See what happens in the game\n",
    "- Get rewards for hitting the ball\n",
    "- Get penalties for missing the ball\n",
    "- Gradually improve its strategy through experience\n",
    "\n",
    "## The Key Components\n",
    "\n",
    "Let's break down the essential parts of our reinforcement learning system:\n",
    "\n",
    "1. **Agent**: The AI that controls the paddle\n",
    "2. **Environment**: The Pong game\n",
    "3. **State**: What our agent can observe about the game\n",
    "   - Ball x-position\n",
    "   - Ball y-position\n",
    "   - Paddle y-position\n",
    "   - Ball x-velocity\n",
    "   - Ball y-velocity\n",
    "4. **Actions**: What our agent can do\n",
    "   - Move paddle up\n",
    "   - Stay in place\n",
    "   - Move paddle down\n",
    "5. **Reward**: The feedback our agent receives\n",
    "   - Positive reward (+1) for hitting the ball\n",
    "   - Negative reward (-1) for missing the ball\n",
    "   - Small \"shaping\" rewards to guide learning\n",
    "\n",
    "\n",
    "![RL](https://upload.wikimedia.org/wikipedia/commons/1/1b/Reinforcement_learning_diagram.svg)\n",
    "\n",
    "\n",
    "## The Learning Loop\n",
    "\n",
    "Here's how the learning process works:\n",
    "\n",
    "1. The agent observes the current state of the game\n",
    "2. Based on this state, it chooses an action (move up, stay, or move down)\n",
    "3. The game updates (the ball and paddle move)\n",
    "4. The agent receives a reward\n",
    "5. The agent observes the new state\n",
    "6. Repeat until the game ends\n",
    "7. After the game ends, the agent learns from what happened\n",
    "\n",
    "This cycle happens over and over - thousands of times - as the agent gradually improves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUT HOW DO WE DO THIS?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to do Reinforcement learning. It all hinges on the algorithm used for the training. \n",
    "\n",
    "- Do we know how to calculate the rewards? \n",
    "- Or the expected rewards for all possible actions? \n",
    "- Is it even possible?\n",
    "- What is the thing that learns? A genetic algorithm? A Neural Network? ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![banner](https://i.imgur.com/SlupuVC.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEHOLD AN ARTIFICIAL NEURON!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60b77a5d536544f6a1ed704aec8317e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='Input', max=5.0, min=-5.0), FloatSlider(value=1.0, d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_neuron(input_value=1.0, weight=1.0, bias=0.0)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from ipywidgets import interact, FloatSlider\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_neuron(input_value=1.0, weight=1.0, bias=0.0):\n",
    "    # Compute output using the neuron formula\n",
    "    output = input_value * weight + bias\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.set_xlim(-1.5, 5.5)\n",
    "    ax.set_ylim(-2, 4.5)\n",
    "    ax.axis('off')\n",
    "    ax.set_aspect('equal', 'box') # Ensure circles are not squished\n",
    "\n",
    "    # Add formula text\n",
    "    formula_text = f\"Formula: output = (input × weight) + bias\\n\" \\\n",
    "                   f\"         = ({input_value:.1f} × {weight:.1f}) + {bias:.1f}\\n\" \\\n",
    "                   f\"         = {output:.2f}\"\n",
    "    ax.text(2.5, 4.2, formula_text, ha='center', va='top', fontsize=12, \n",
    "            bbox=dict(facecolor='white', alpha=0.9))\n",
    "\n",
    "    # Draw input node\n",
    "    ax.text(-1.2, 1.3, f\"Input\\n{input_value:0.2f}\", fontsize=12, ha=\"center\")\n",
    "    plt.plot([-1.4, -0.7], [1.0, 1.0], color='gray', lw=2, linestyle='--')\n",
    "    \n",
    "    # Draw neuron\n",
    "    circle = plt.Circle((2.0, 1.0), 1, color='skyblue', ec='k', zorder=2)\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(2.0, 1.0, \"Neuron\", fontsize=12, ha=\"center\", va=\"center\")\n",
    "    \n",
    "    # Draw bias\n",
    "    ax.annotate(\"\", xy=(2.0, 2), xytext=(2.0, 2.7),\n",
    "                arrowprops=dict(arrowstyle=\"->\", color=\"red\", lw=2))\n",
    "    ax.text(2.0, 2.8, f\"Bias: {bias:0.2f}\", color=\"red\", ha=\"center\", fontsize=12)\n",
    "    \n",
    "    # Draw weight\n",
    "    ax.annotate(\"\", xy=(1.0, 1.0), xytext=(-0.7, 1.0),\n",
    "                arrowprops=dict(arrowstyle=\"->\", color=\"blue\", lw=2))\n",
    "    ax.text(-0.1, 1.1, f\"Weight: {weight:0.2f}\", color=\"blue\", ha=\"center\", fontsize=12)\n",
    "    \n",
    "    # Draw output\n",
    "    ax.annotate(\"\", xy=(3, 1.0), xytext=(5.2, 1.0),\n",
    "                arrowprops=dict(arrowstyle=\"<-\", color=\"green\", lw=2))\n",
    "    ax.text(4.2, 1.1, f\"Output: {output:0.2f}\", color=\"green\", ha=\"center\", fontsize=12)\n",
    "    \n",
    "    ax.set_title(\"Single Neuron with Linear Activation\", fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "# Create interactive widget\n",
    "interact(plot_neuron,\n",
    "         input_value=FloatSlider(min=-5, max=5, step=0.1, value=1.0, description=\"Input\"),\n",
    "         weight=FloatSlider(min=-5, max=5, step=0.1, value=1.0, description=\"Weight\"),\n",
    "         bias=FloatSlider(min=-5, max=5, step=0.1, value=0.0, description=\"Bias\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A network\n",
    "\n",
    "Several of those neurons (billions in the case of modern AI systems) are put together in a network. Usually in layers that connect to each other, each neuron multiplying, adding and sending its output forward to more neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eafe0590bdf4b2a9311abbc22bb46e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(FloatSlider(value=1.0, description='x₁', max=2.0, min=-2.0), FloatSlider(value=1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c4376e5ae92417ca51fd65afc410cdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Imports\n",
    "from ipywidgets import FloatSlider, VBox, HBox, interactive_output\n",
    "from IPython.display import display\n",
    "\n",
    "def plot_network(x1, x2, w1_00, w1_10, w1_01, w1_11, w2_0, w2_1):\n",
    "    # Compute the forward pass\n",
    "    h1 = x1 * w1_00 + x2 * w1_10\n",
    "    h2 = x1 * w1_01 + x2 * w1_11\n",
    "    output = h1 * w2_0 + h2 * w2_1\n",
    "\n",
    "    # Set up the figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.set_xlim(-1.5, 5)\n",
    "    ax.set_ylim(-1.5, 3)\n",
    "    ax.axis('off') \n",
    "    ax.set_aspect('equal', 'box') # Ensure circles are not squished\n",
    "\n",
    "    # Define positions for nodes in each layer\n",
    "    positions = {\n",
    "        \"x1\": (0, 1.5),\n",
    "        \"x2\": (0, 0.5),\n",
    "        \"h1\": (2, 1.5),\n",
    "        \"h2\": (2, 0.5),\n",
    "        \"output\": (4, 1)\n",
    "    }\n",
    "\n",
    "    # Function to draw each node: a circle, with the label and node value inside\n",
    "    def draw_node(pos, value, label, color):\n",
    "        circle = plt.Circle(pos, 0.2, color=color, ec='k', zorder=5)\n",
    "        ax.add_patch(circle)\n",
    "        ax.text(pos[0], pos[1], f\"{label}\\n{value:.2f}\", \n",
    "                ha='center', va='center', fontsize=10, zorder=6)\n",
    "\n",
    "    # Draw nodes for each layer\n",
    "    draw_node(positions[\"x1\"], x1, \"x₁\", 'lightyellow')\n",
    "    draw_node(positions[\"x2\"], x2, \"x₂\", 'lightyellow')\n",
    "    draw_node(positions[\"h1\"], h1, \"h₁\", 'skyblue')\n",
    "    draw_node(positions[\"h2\"], h2, \"h₂\", 'skyblue')\n",
    "    draw_node(positions[\"output\"], output, \"ŷ\", 'lightgreen')\n",
    "\n",
    "    # Draw layer labels above the nodes\n",
    "    ax.text(positions[\"x1\"][0], positions[\"x1\"][1] + 0.6, \"Input Layer\",\n",
    "            ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "    ax.text(positions[\"h1\"][0], positions[\"h1\"][1] + 0.6, \"Hidden Layer\",\n",
    "            ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "    ax.text(positions[\"output\"][0], positions[\"output\"][1] + 0.6, \"Output Layer\",\n",
    "            ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "    # Also add a clear summary of input and output values on the sides\n",
    "    ax.text(-1.3, 1, f\"Inputs:\\n x₁ = {x1:.2f}\\n x₂ = {x2:.2f}\",\n",
    "            fontsize=11, ha='center', va='center',\n",
    "            bbox=dict(facecolor='white', alpha=0.9, edgecolor='gray'))\n",
    "    ax.text(4.8, 1, f\"Output:\\n ŷ = {output:.2f}\",\n",
    "            fontsize=11, ha='center', va='center',\n",
    "            bbox=dict(facecolor='white', alpha=0.9, edgecolor='gray'))\n",
    "\n",
    "    # Define the connections with their labels. Each connection is a tuple:\n",
    "    # (start_node, end_node, current weight value, weight label)\n",
    "    connections = [\n",
    "        (\"x1\", \"h1\", w1_00, \"w₁₀₀\"),\n",
    "        (\"x2\", \"h1\", w1_10, \"w₁₁₀\"),\n",
    "        (\"x1\", \"h2\", w1_01, \"w₁₀₁\"),\n",
    "        (\"x2\", \"h2\", w1_11, \"w₁₁₁\"),\n",
    "        (\"h1\", \"output\", w2_0, \"w₂₀\"),\n",
    "        (\"h2\", \"output\", w2_1, \"w₂₁\"),\n",
    "    ]\n",
    "    \n",
    "    # Function to draw an arrow (connection) with the connection label and weight value\n",
    "    def draw_arrow(start, end, weight, wt_label):\n",
    "        start_pos = np.array(positions[start])\n",
    "        end_pos = np.array(positions[end])\n",
    "        vector = end_pos - start_pos\n",
    "        length = np.linalg.norm(vector)\n",
    "        direction = vector / length\n",
    "        \n",
    "        # Adjust start and end positions so the arrow doesn't overlap the node circles\n",
    "        start_adjust = start_pos + direction * 0.25\n",
    "        end_adjust = end_pos - direction * 0.25\n",
    "        \n",
    "        # Draw arrow between nodes\n",
    "        ax.annotate(\"\",\n",
    "                    xy=end_adjust,\n",
    "                    xytext=start_adjust,\n",
    "                    arrowprops=dict(arrowstyle=\"->\", color=\"gray\", lw=1.5),\n",
    "                    zorder=3)\n",
    "        # Place a label for the connection: show the weight variable and value\n",
    "        midpoint = (start_adjust + end_adjust) / 2.0\n",
    "        # Use a slight offset for clarity\n",
    "        offset = np.array([0.0, 0.15])\n",
    "        ax.text(midpoint[0] + offset[0], midpoint[1] + offset[1],\n",
    "                f\"{wt_label}\\n{weight:.2f}\", fontsize=9, color=\"red\",\n",
    "                ha='center', va='center', bbox=dict(facecolor='white', alpha=0.8, edgecolor='none'))\n",
    "\n",
    "    # Draw all connection arrows with labels\n",
    "    for start, end, weight, wt_label in connections:\n",
    "        draw_arrow(start, end, weight, wt_label)\n",
    "\n",
    "    # Place an explanation text block on the upper right, if desired\n",
    "    explanation_text = (\n",
    "        \"Feedforward Computation:\\n\"\n",
    "        \"1. Inputs x₁ and x₂ are each multiplied by their connection weights.\\n\"\n",
    "        \"2. Hidden neurons sum these weighted inputs (h₁, h₂).\\n\"\n",
    "        \"3. Hidden outputs are multiplied by output weights and summed to form ŷ.\"\n",
    "    )\n",
    "    ax.text(4.2, 2.7, explanation_text, fontsize=10,\n",
    "            bbox=dict(facecolor='white', edgecolor='gray', alpha=0.8),\n",
    "            ha='left', va='top')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "### Create Interactive Widgets ###\n",
    "\n",
    "# Input sliders for x1 and x2\n",
    "slider_x1 = FloatSlider(min=-2, max=2, step=0.1, value=1.0, description=\"x₁\")\n",
    "slider_x2 = FloatSlider(min=-2, max=2, step=0.1, value=1.0, description=\"x₂\")\n",
    "\n",
    "# Sliders for weights connecting inputs to the hidden layer\n",
    "slider_w1_00 = FloatSlider(min=-2, max=2, step=0.1, value=1.0, description=\"w₁₀₀\")\n",
    "slider_w1_10 = FloatSlider(min=-2, max=2, step=0.1, value=1.0, description=\"w₁₁₀\")\n",
    "slider_w1_01 = FloatSlider(min=-2, max=2, step=0.1, value=1.0, description=\"w₁₀₁\")\n",
    "slider_w1_11 = FloatSlider(min=-2, max=2, step=0.1, value=1.0, description=\"w₁₁₁\")\n",
    "\n",
    "# Sliders for weights connecting the hidden layer to the output\n",
    "slider_w2_0 = FloatSlider(min=-2, max=2, step=0.1, value=1.0, description=\"w₂₀\")\n",
    "slider_w2_1 = FloatSlider(min=-2, max=2, step=0.1, value=1.0, description=\"w₂₁\")\n",
    "\n",
    "# Organize the slider layout\n",
    "inputs_box = HBox([slider_x1, slider_x2])\n",
    "weights_input_hidden = HBox([slider_w1_00, slider_w1_10, slider_w1_01, slider_w1_11])\n",
    "weights_hidden_output = HBox([slider_w2_0, slider_w2_1])\n",
    "ui = VBox([inputs_box, weights_input_hidden, weights_hidden_output])\n",
    "\n",
    "# Set up the interactive output\n",
    "out = interactive_output(plot_network, {\n",
    "    \"x1\": slider_x1,\n",
    "    \"x2\": slider_x2,\n",
    "    \"w1_00\": slider_w1_00,\n",
    "    \"w1_10\": slider_w1_10,\n",
    "    \"w1_01\": slider_w1_01,\n",
    "    \"w1_11\": slider_w1_11,\n",
    "    \"w2_0\": slider_w2_0,\n",
    "    \"w2_1\": slider_w2_1,\n",
    "})\n",
    "\n",
    "# Display the interactive UI and plot\n",
    "display(ui, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training vs. Playing: Two Different Modes\n",
    "\n",
    "It's important to understand the two modes of our agent:\n",
    "\n",
    "### Training Mode\n",
    "- Agent chooses actions randomly at first, based on probabilities from the network\n",
    "- It records everything that happens (states, actions, rewards)\n",
    "- After each game, it updates its neural network to improve\n",
    "- This involves exploration (trying new things)\n",
    "\n",
    "### Playing Mode\n",
    "- Agent always chooses the action with highest probability\n",
    "- No more randomness or exploration\n",
    "- No more learning or updates\n",
    "- Just using what it has learned\n",
    "\n",
    "We spend most of our time in training mode, then switch to playing mode when the agent is ready.\n",
    "\n",
    "## The REINFORCE Algorithm: Learning from Success and Failure\n",
    "\n",
    "Now let's understand how our agent actually learns. We're using an algorithm called REINFORCE, which we'll explain step-by-step:\n",
    "\n",
    "### Step 1: Play a Complete Game\n",
    "\n",
    "The agent plays a full game of Pong until it misses the ball (game over). During this game, we record:\n",
    "- Each state it observed\n",
    "- Each action it took\n",
    "- Each reward it received\n",
    "\n",
    "Let's say our agent played a game that lasted 50 moves before missing the ball. We now have 50 (state, action, reward) tuples stored in memory.\n",
    "\n",
    "### Step 2: Calculate the \"Returns\"\n",
    "\n",
    "We need to know which actions were actually good in the long run. This is tricky because sometimes an action might look good immediately but lead to failure later.\n",
    "\n",
    "To solve this, we calculate the \"return\" for each step - essentially the total future reward from that point onwards, with future rewards discounted (valued less than immediate rewards).\n",
    "\n",
    "For each step t, we calculate:\n",
    "```\n",
    "Return(t) = Reward(t) + gamma * Reward(t+1) + gamma² * Reward(t+2) + ...\n",
    "```\n",
    "\n",
    "Where `gamma` is a number between 0 and 1 that determines how much we care about future rewards.\n",
    "\n",
    "#### Example:\n",
    "If our rewards were [0, 0, 0, 1, 0, 0, -1] and gamma is 0.9:\n",
    "- Return at step 6 = -1\n",
    "- Return at step 5 = 0 + 0.9 * (-1) = -0.9\n",
    "- Return at step 4 = 0 + 0.9 * (-0.9) = -0.81\n",
    "- Return at step 3 = 1 + 0.9 * (-0.81) = 0.271\n",
    "- ...and so on\n",
    "\n",
    "This gives us a better measure of how good each action really was.\n",
    "\n",
    "\n",
    "\n",
    "### Step 3: Update the Policy Network\n",
    "\n",
    "Now comes the crucial part - we need to adjust our neural network to make good actions more likely and bad actions less likely in the future.\n",
    "\n",
    "For each (state, action, return) tuple:\n",
    "1. Feed the state into the network to get the current probabilities\n",
    "2. Increase the probability of the action taken if the return was positive\n",
    "3. Decrease the probability of the action taken if the return was negative\n",
    "\n",
    "#### How Weights Actually Change\n",
    "\n",
    "This is where we need to understand how neural networks learn:\n",
    "\n",
    "1. Each connection in our neural network has a \"weight\" - just a number that determines how strong that connection is.\n",
    "2. These weights determine the final probabilities output by the network.\n",
    "3. To make an action more likely, we need to adjust the weights that led to that action.\n",
    "\n",
    "Let's break this down with a simple example:\n",
    "\n",
    "Imagine our network gave these probabilities for a particular state:\n",
    "- UP: 30%\n",
    "- STAY: 50%\n",
    "- DOWN: 20%\n",
    "\n",
    "The agent selected STAY (based on these probabilities), and this eventually led to a positive return of 0.8.\n",
    "\n",
    "We want to adjust our network to make STAY even more likely in this situation next time. The math works out such that:\n",
    "- Weights that contributed to the STAY probability get increased\n",
    "- The larger the return (0.8 in this case), the larger the increase\n",
    "- Weights that didn't contribute to STAY don't change much\n",
    "\n",
    "\n",
    "The technical term for this process is \"gradient ascent on the policy parameters\" - but you can think of it as \"tweak the weights to make good actions more likely.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global settings.\n",
    "WIDTH = 600.0\n",
    "HEIGHT = 400.0\n",
    "PADDLE_WIDTH = 20.0\n",
    "PADDLE_HEIGHT = 80.0\n",
    "BALL_RADIUS = 10.0\n",
    "BALL_SPEED = 9.0\n",
    "PADDLE_MOVE_SPEED = 9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import threading\n",
    "import numpy as np\n",
    "from ipycanvas import Canvas\n",
    "import ipywidgets as widgets\n",
    "from ipyevents import Event\n",
    "from IPython.display import display\n",
    "\n",
    "def pong_step(state, action):\n",
    "    \"\"\"\n",
    "    Update ball and right-paddle state.\n",
    "    \n",
    "    state: [ball_x, ball_y, paddle_y, ball_dx, ball_dy]\n",
    "    action (for right paddle): 0 = up, 1 = none, 2 = down.\n",
    "    \n",
    "    Returns a list of native Python floats.\n",
    "    \"\"\"\n",
    "    ball_x, ball_y, paddle_y, ball_dx, ball_dy = state\n",
    "\n",
    "    # Update AI paddle position (right paddle) based on action.\n",
    "    if action == 0:\n",
    "        paddle_y = max(0.0, paddle_y - PADDLE_MOVE_SPEED)\n",
    "    elif action == 2:\n",
    "        paddle_y = min(HEIGHT - PADDLE_HEIGHT, paddle_y + PADDLE_MOVE_SPEED)\n",
    "\n",
    "    # Update ball position.\n",
    "    ball_x += ball_dx\n",
    "    ball_y += ball_dy\n",
    "\n",
    "    # Bounce off top and bottom walls.\n",
    "    if ball_y - BALL_RADIUS < 0:\n",
    "        ball_y = BALL_RADIUS\n",
    "        ball_dy = abs(ball_dy)\n",
    "    elif ball_y + BALL_RADIUS > HEIGHT:\n",
    "        ball_y = HEIGHT - BALL_RADIUS\n",
    "        ball_dy = -abs(ball_dy)\n",
    "\n",
    "    # Handle collision with the right (AI) paddle.\n",
    "    if ball_x + BALL_RADIUS >= WIDTH - PADDLE_WIDTH:\n",
    "        # If ball hits paddle, bounce back.\n",
    "        if paddle_y <= ball_y <= (paddle_y + PADDLE_HEIGHT):\n",
    "            ball_x = WIDTH - PADDLE_WIDTH - BALL_RADIUS\n",
    "            ball_dx = -abs(ball_dx)\n",
    "        else:\n",
    "            # If the paddle missed, reset the ball and the right paddle.\n",
    "            ball_x = WIDTH / 2.0\n",
    "            ball_y = random.uniform(BALL_RADIUS, HEIGHT - BALL_RADIUS)\n",
    "            ball_dx = BALL_SPEED\n",
    "            ball_dy = BALL_SPEED\n",
    "            paddle_y = (HEIGHT - PADDLE_HEIGHT) / 2.0\n",
    "\n",
    "    # (Left wall collision handled in game loop)\n",
    "    return [float(ball_x), float(ball_y), float(paddle_y), float(ball_dx), float(ball_dy)]\n",
    "\n",
    "\n",
    "class PongGame:\n",
    "    def __init__(self, ai_function):\n",
    "        \"\"\"\n",
    "        Initialize the Pong game.\n",
    "        \n",
    "        ai_function(ball_x, ball_y, paddle_y, ball_dx, ball_dy)\n",
    "            should return one of: \"up\", \"none\", or \"down\" for the right paddle.\n",
    "        \"\"\"\n",
    "        # Left (player) paddle and ball state.\n",
    "        self.left_paddle_y = (HEIGHT - PADDLE_HEIGHT) / 2.0\n",
    "        self.right_paddle_y = (HEIGHT - PADDLE_HEIGHT) / 2.0\n",
    "        self.ball_x = WIDTH / 2.0\n",
    "        self.ball_y = random.uniform(BALL_RADIUS, HEIGHT - BALL_RADIUS)\n",
    "        self.ball_dx = BALL_SPEED\n",
    "        self.ball_dy = BALL_SPEED\n",
    "\n",
    "        self.ai_function = ai_function\n",
    "\n",
    "        # Movement flags for the left paddle.\n",
    "        self.left_up_active = False\n",
    "        self.left_down_active = False\n",
    "\n",
    "        self.running = False\n",
    "\n",
    "        self._create_widgets()\n",
    "\n",
    "    def _create_widgets(self):\n",
    "        # Create the game canvas.\n",
    "        self.canvas = Canvas(width=WIDTH, height=HEIGHT)\n",
    "        display(self.canvas)\n",
    "        \n",
    "        # Create control buttons.\n",
    "        self.btn_left_up = widgets.Button(\n",
    "            description=\"Left UP\", layout=widgets.Layout(width='80px'))\n",
    "        self.btn_left_down = widgets.Button(\n",
    "            description=\"Left DOWN\", layout=widgets.Layout(width='80px'))\n",
    "        self.btn_stop = widgets.Button(\n",
    "            description=\"STOP GAME\", layout=widgets.Layout(width='100px', height='40px'),\n",
    "            button_style='danger')\n",
    "        \n",
    "        # Set up ipyevents on the left paddle buttons for mousedown/up/leave.\n",
    "        event_up = Event(source=self.btn_left_up, watched_events=['mousedown', 'mouseup', 'mouseleave'])\n",
    "        event_up.on_dom_event(self._handle_left_up)\n",
    "        event_down = Event(source=self.btn_left_down, watched_events=['mousedown', 'mouseup', 'mouseleave'])\n",
    "        event_down.on_dom_event(self._handle_left_down)\n",
    "\n",
    "        # Stop button uses normal on_click.\n",
    "        self.btn_stop.on_click(self._stop_game)\n",
    "        \n",
    "        # Display control buttons.\n",
    "        controls = widgets.VBox([widgets.HBox([self.btn_left_up, self.btn_left_down]), self.btn_stop])\n",
    "        display(controls)\n",
    "    \n",
    "    def _handle_left_up(self, event):\n",
    "        # When the up button is pressed, set the flag; released/leave clears it.\n",
    "        if event['type'] == 'mousedown':\n",
    "            self.left_up_active = True\n",
    "        elif event['type'] in ['mouseup', 'mouseleave']:\n",
    "            self.left_up_active = False\n",
    "    \n",
    "    def _handle_left_down(self, event):\n",
    "        # When the down button is pressed, set the flag; released/leave clears it.\n",
    "        if event['type'] == 'mousedown':\n",
    "            self.left_down_active = True\n",
    "        elif event['type'] in ['mouseup', 'mouseleave']:\n",
    "            self.left_down_active = False\n",
    "    \n",
    "    def _draw(self):\n",
    "        # Clear the canvas and redraw the game objects.\n",
    "        self.canvas.clear()\n",
    "\n",
    "        self.canvas.fill_style = 'black'\n",
    "        self.canvas.fill_circle(self.ball_x, self.ball_y, BALL_RADIUS)\n",
    "        \n",
    "        # Draw background.\n",
    "        self.canvas.fill_style = 'white'\n",
    "        self.canvas.fill_rect(0, 0, WIDTH, HEIGHT)\n",
    "        \n",
    "        # Draw left (player) paddle.\n",
    "        self.canvas.fill_style = 'blue'\n",
    "        self.canvas.fill_rect(0, self.left_paddle_y, PADDLE_WIDTH, PADDLE_HEIGHT)\n",
    "        \n",
    "        # Draw right (AI) paddle.\n",
    "        self.canvas.fill_style = 'red'\n",
    "        self.canvas.fill_rect(WIDTH - PADDLE_WIDTH, self.right_paddle_y, PADDLE_WIDTH, PADDLE_HEIGHT)\n",
    "        \n",
    "        # Draw ball.\n",
    "        self.canvas.fill_style = 'black'\n",
    "        self.canvas.fill_circle(self.ball_x, self.ball_y, BALL_RADIUS)\n",
    "    \n",
    "    def _reset_ball(self):\n",
    "        # Reset the ball to the center with a random vertical position.\n",
    "        self.ball_x = WIDTH / 2.0\n",
    "        self.ball_y = random.uniform(BALL_RADIUS, HEIGHT - BALL_RADIUS)\n",
    "        self.ball_dx = BALL_SPEED\n",
    "        self.ball_dy = BALL_SPEED\n",
    "        self.right_paddle_y = (HEIGHT - PADDLE_HEIGHT) / 2.0\n",
    "\n",
    "    def game_loop(self):\n",
    "        fps_delay = 1.0 / 30.0  # approximately 30 FPS\n",
    "        mapping = {\"up\": 0, \"none\": 1, \"down\": 2}\n",
    "        while self.running:\n",
    "            # Move the left paddle based on button flags.\n",
    "            if self.left_up_active:\n",
    "                self.left_paddle_y = max(0.0, self.left_paddle_y - PADDLE_MOVE_SPEED)\n",
    "            if self.left_down_active:\n",
    "                self.left_paddle_y = min(HEIGHT - PADDLE_HEIGHT, self.left_paddle_y + PADDLE_MOVE_SPEED)\n",
    "            \n",
    "            # Build the game state for the ball and right paddle.\n",
    "            state = [self.ball_x, self.ball_y, self.right_paddle_y, self.ball_dx, self.ball_dy]\n",
    "\n",
    "            # Get the AI action for the right paddle.\n",
    "            ai_action = self.ai_function(self.ball_x, self.ball_y, self.right_paddle_y, self.ball_dx, self.ball_dy)\n",
    "            action_int = mapping.get(ai_action, 1)\n",
    "\n",
    "            # Update ball position and the right paddle using pong_step.\n",
    "            new_state = pong_step(state, action_int)\n",
    "            self.ball_x, self.ball_y, self.right_paddle_y, self.ball_dx, self.ball_dy = new_state\n",
    "            \n",
    "            # Check collision with the left (player) paddle.\n",
    "            if self.ball_x - BALL_RADIUS <= PADDLE_WIDTH:\n",
    "                if self.left_paddle_y <= self.ball_y <= (self.left_paddle_y + PADDLE_HEIGHT):\n",
    "                    # Bounce the ball off the player's paddle.\n",
    "                    self.ball_x = PADDLE_WIDTH + BALL_RADIUS\n",
    "                    self.ball_dx = abs(self.ball_dx)\n",
    "                else:\n",
    "                    # The player missed: reset the ball.\n",
    "                    self._reset_ball()\n",
    "            \n",
    "            self._draw()\n",
    "            time.sleep(fps_delay)\n",
    "    \n",
    "    def start(self):\n",
    "        self.running = True\n",
    "        # Run the game loop in a separate thread to free the UI thread.\n",
    "        self.thread = threading.Thread(target=self.game_loop, daemon=True)\n",
    "        self.thread.start()\n",
    "    \n",
    "    def _stop_game(self, _):\n",
    "        self.running = False\n",
    "        self.btn_stop.description = \"Stopped\"\n",
    "        self.btn_stop.disabled = True\n",
    "        self.left_up_active = False\n",
    "        self.left_down_active = False\n",
    "\n",
    "def start_game(ai_function):\n",
    "    \"\"\"\n",
    "    Initialize and start the Pong game.\n",
    "    \n",
    "    Provide an ai_function(ball_x, ball_y, paddle_y, ball_dx, ball_dy)\n",
    "    that returns \"up\", \"none\", or \"down\" for controlling the right paddle.\n",
    "    \"\"\"\n",
    "    game = PongGame(ai_function)\n",
    "    game.start()\n",
    "    return game\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6774542fdded4aec8c1f2a8c5ea10eac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(height=400, width=600)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26a35efede2c454697b0be815b053330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Button(description='Left UP', layout=Layout(width='80px'), style=ButtonStyle()),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<__main__.PongGame at 0x39a6d9c40>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is how we use it.\n",
    "\n",
    "# --- Example AI Function ---\n",
    "def simple_ai(ball_x, ball_y, paddle_y, ball_dx, ball_dy):\n",
    "    \"\"\"\n",
    "    A basic AI: move the paddle up or down so that its center follows the ball.\n",
    "    \"\"\"\n",
    "    paddle_center = paddle_y + 30  # paddle_height/2, here paddle_height is 60.\n",
    "    if ball_y < paddle_center:  \n",
    "        return \"up\"\n",
    "    elif ball_y > paddle_center:\n",
    "        return \"down\"\n",
    "    else:\n",
    "        return \"none\"\n",
    "\n",
    "\n",
    "# --- Start the Game ---\n",
    "# Pass the AI function you want to use.\n",
    "start_game(simple_ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100/500: Steps= 30, Total Reward= 11.53, Max Steps reached= 408\n",
      "Episode 200/500: Steps= 30, Total Reward= 17.86, Max Steps reached= 282\n",
      "Episode 300/500: Steps= 30, Total Reward= 13.06, Max Steps reached= 156\n",
      "Episode 400/500: Steps= 156, Total Reward= 77.11, Max Steps reached= 156\n",
      "Episode 500/500: Steps= 30, Total Reward= 10.53, Max Steps reached= 408\n"
     ]
    }
   ],
   "source": [
    "# %% Cell B: RL Training Environment\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "def rl_step(state, action):\n",
    "    \"\"\"\n",
    "    Update the physics and compute reward and done.\n",
    "    State: [ball_x, ball_y, paddle_y, ball_dx, ball_dy]\n",
    "    Action: 0 = up, 1 = no move, 2 = down.\n",
    "    \n",
    "    Reward shaping and done determination are present:\n",
    "      - A reward of +1 (plus some shaping) is given upon a paddle hit.\n",
    "      - A reward of -1 is given and done=True when the paddle misses.\n",
    "    \"\"\"\n",
    "    ball_x, ball_y, paddle_y, ball_dx, ball_dy = state\n",
    "\n",
    "    # Update right paddle.\n",
    "    if action == 0:\n",
    "        paddle_y = max(0.0, paddle_y - PADDLE_MOVE_SPEED)\n",
    "    elif action == 2:\n",
    "        paddle_y = min(HEIGHT - PADDLE_HEIGHT, paddle_y + PADDLE_MOVE_SPEED)\n",
    "    \n",
    "    # Update ball position.\n",
    "    ball_x += ball_dx\n",
    "    ball_y += ball_dy\n",
    "    \n",
    "    # Bounce off top and bottom.\n",
    "    if ball_y - BALL_RADIUS < 0:\n",
    "        ball_y = BALL_RADIUS\n",
    "        ball_dy = abs(ball_dy)\n",
    "    if ball_y + BALL_RADIUS > HEIGHT:\n",
    "        ball_y = HEIGHT - BALL_RADIUS\n",
    "        ball_dy = -abs(ball_dy)\n",
    "    \n",
    "    # Reward shaping: reward is a function of how close the paddle is to ball center.\n",
    "    paddle_center = paddle_y + PADDLE_HEIGHT / 2.0\n",
    "    shaping_factor= 0.8\n",
    "    shaping_reward = (1 - abs(paddle_center - ball_y)/HEIGHT) * shaping_factor\n",
    "    if action == 1:\n",
    "        shaping_reward -= 0.1  # encourage movement.\n",
    "    reward = shaping_reward \n",
    "\n",
    "    done = False\n",
    "    \n",
    "    # Collision with right paddle.\n",
    "    if ball_x + BALL_RADIUS >= WIDTH - PADDLE_WIDTH:\n",
    "        if paddle_y <= ball_y <= (paddle_y + PADDLE_HEIGHT):\n",
    "            # Successful hit.\n",
    "            reward = shaping_reward + 1.0\n",
    "            ball_x = WIDTH - PADDLE_WIDTH - BALL_RADIUS  # reposition\n",
    "            ball_dx = -abs(ball_dx)\n",
    "        else:\n",
    "            reward = shaping_reward - 1.0\n",
    "            done = True\n",
    "\n",
    "    # Bounce off left wall.\n",
    "    if ball_x - BALL_RADIUS <= 0:\n",
    "        ball_x = BALL_RADIUS\n",
    "        ball_dx = abs(ball_dx)\n",
    "\n",
    "    new_state = np.array([ball_x, ball_y, paddle_y, ball_dx, ball_dy], dtype=np.float32)\n",
    "    return new_state, reward, done\n",
    "\n",
    "# --------------------------------------------------\n",
    "# RL Agent (Policy Network using REINFORCE)\n",
    "# --------------------------------------------------\n",
    "class RLAgent:\n",
    "    def __init__(self, learning_rate=5e-3, gamma=0.76):\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.model = keras.Sequential([\n",
    "            layers.Input(shape=(5,)),\n",
    "            layers.Dense(8, activation='relu'),\n",
    "            layers.Dense(3, activation='softmax')\n",
    "        ])\n",
    "        self.optimizer = tf.keras.optimizers.legacy.Adam(learning_rate)\n",
    "        \n",
    "        # Buffers to store transitions.\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "    \n",
    "    def _normalize_state(self, state):\n",
    "        # Normalize each component for training.\n",
    "        return np.array([\n",
    "            state[0] / WIDTH,\n",
    "            state[1] / HEIGHT,\n",
    "            state[2] / HEIGHT,\n",
    "            state[3] / BALL_SPEED,\n",
    "            state[4] / BALL_SPEED,\n",
    "        ], dtype=np.float32)\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        norm_state = self._normalize_state(state).reshape(1, -1)\n",
    "        probs = self.model(norm_state).numpy().flatten()\n",
    "        action = np.random.choice(3, p=probs)\n",
    "        self.states.append(norm_state)\n",
    "        self.actions.append(action)\n",
    "        return action\n",
    "    \n",
    "    def store_reward(self, reward):\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def finish_episode(self):\n",
    "        \"\"\"Use a REINFORCE update.\"\"\"\n",
    "        discounted = np.zeros_like(self.rewards, dtype=np.float32)\n",
    "        cumulative = 0.0\n",
    "        for i in reversed(range(len(self.rewards))):\n",
    "            cumulative = self.rewards[i] + self.gamma * cumulative\n",
    "            discounted[i] = cumulative\n",
    "\n",
    "        baseline = np.mean(discounted)\n",
    "        discounted -= baseline\n",
    "        \n",
    "        states = np.concatenate(self.states, axis=0)\n",
    "        actions = np.array(self.actions)\n",
    "        rewards = discounted\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            probs = self.model(states, training=True)\n",
    "            action_mask = tf.one_hot(actions, 3)\n",
    "            log_probs = tf.math.log(tf.reduce_sum(probs * action_mask, axis=1) + 1e-8)\n",
    "            loss = -tf.reduce_mean(log_probs * rewards)\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        \n",
    "        # Clear the buffers.\n",
    "        self.states, self.actions, self.rewards = [], [], []\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        norm_state = self._normalize_state(state).reshape(1, -1)\n",
    "        probs = self.model(norm_state).numpy().flatten()\n",
    "        return np.argmax(probs)\n",
    "\n",
    "def train_agent(num_episodes=1000):\n",
    "    agent = RLAgent()\n",
    "    total_rewards = []\n",
    "\n",
    "    max_steps_reached = 0\n",
    "    for i in range(num_episodes):\n",
    "        # Randomize initial conditions for each episode.\n",
    "        ball_y_random = np.random.uniform(BALL_RADIUS, HEIGHT - BALL_RADIUS)\n",
    "        paddle_y_random = np.random.uniform(0, HEIGHT - PADDLE_HEIGHT)\n",
    "        \n",
    "        state = np.array([\n",
    "            WIDTH / 2.0, \n",
    "            ball_y_random,\n",
    "            paddle_y_random,\n",
    "            BALL_SPEED,\n",
    "            BALL_SPEED\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        episode_reward = 0.0\n",
    "        done = False\n",
    "        step = 0\n",
    "        \n",
    "        max_steps = 500  # or set any desired number of iterations\n",
    "        while not done and step < max_steps:\n",
    "            action = agent.choose_action(state)\n",
    "            state, reward, done = rl_step(state, action)\n",
    "            agent.store_reward(reward)\n",
    "            episode_reward += reward\n",
    "            step += 1\n",
    "        \n",
    "        agent.finish_episode()\n",
    "        total_rewards.append(episode_reward)\n",
    "        \n",
    "\n",
    "        max_steps_reached = max(max_steps_reached, step)\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Episode {i+1}/{num_episodes}: Steps= {step}, Total Reward= {episode_reward:.2f}, Max Steps reached= {max_steps_reached}\")\n",
    "            max_steps_reached = 0\n",
    "    \n",
    "    return agent\n",
    "\n",
    "# Train the agent.\n",
    "trained_agent = train_agent(num_episodes=500)\n",
    "\n",
    "# Wrap the trained agent into an AI function for gameplay.\n",
    "def trained_ai_function(ball_x, ball_y, paddle_y, ball_dx, ball_dy):\n",
    "    state = np.array([ball_x, ball_y, paddle_y, ball_dx, ball_dy], dtype=np.float32)\n",
    "    action_idx = trained_agent.get_action(state)\n",
    "    mapping = {0: \"up\", 1: \"none\", 2: \"down\"}\n",
    "    return mapping[action_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained agent.\n",
    "#trained_agent.save(\"trained_pong_agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the traioned agent.\n",
    "#trained_agent = keras.models.load_model(\"trained_pong_agent\")\n",
    "#def trained_ai_function(ball_x, ball_y, paddle_y, ball_dx, ball_dy):\n",
    "#    state = np.array([ball_x, ball_y, paddle_y, ball_dx, ball_dy], dtype=np.float32)\n",
    "#    action_probs = trained_agent(state.reshape(1, -1)).numpy().flatten()\n",
    "#    action_idx = np.argmax(action_probs)\n",
    "#    mapping = {0: \"up\", 1: \"none\", 2: \"down\"}\n",
    "#    return mapping[action_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc214b73573a4c0785f8e020288ce493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(height=400, width=600)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22cb286ceaeb48dba3528e006ec3dc0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Button(description='Left UP', layout=Layout(width='80px'), style=ButtonStyle()),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<__main__.PongGame at 0x39a732850>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_game(trained_ai_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting hidden layer: dense_14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "204b11ebc5574074a4efa5d2a34acd5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(FloatSlider(value=300.0, description='Ball X', layout=Layout(width='300px'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@markdown Run to visualize the full trained network\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interactive, HBox, VBox\n",
    "from IPython.display import display\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# --- Ensure that your trained model is built ---\n",
    "# (This dummy call forces the model’s graph to be built.)\n",
    "_dummy = np.zeros((1, 5), dtype=np.float32)\n",
    "_ = trained_agent.model(_dummy)\n",
    "\n",
    "# --- Determine the hidden dense layer ---\n",
    "# Depending on your Keras version the explicit Input layer might not be in model.layers.\n",
    "# In our RLAgent model, if the [Input, Dense, Dense] remains then:\n",
    "#    model.layers[0] is the InputLayer and model.layers[1] is Dense(8)\n",
    "# but in Keras 3 the InputLayer is often omitted in model.layers.\n",
    "#\n",
    "# Check the number of layers and adjust accordingly:\n",
    "if len(trained_agent.model.layers) == 2:\n",
    "    # Only the Dense layers are present.\n",
    "    hidden_layer = trained_agent.model.layers[0]  # Dense(8)\n",
    "elif len(trained_agent.model.layers) >= 3:\n",
    "    # If the Input layer is included.\n",
    "    hidden_layer = trained_agent.model.layers[1]  # Dense(8)\n",
    "else:\n",
    "    hidden_layer = trained_agent.model.layers[0]  # Fallback\n",
    "\n",
    "print(\"Extracting hidden layer:\", hidden_layer.name)\n",
    "\n",
    "def visualize_trained_network(ball_x, ball_y, paddle_y, ball_dx, ball_dy):\n",
    "    # Retrieve network weights.\n",
    "    # Assumed order: [kernel_hidden, bias_hidden, kernel_output, bias_output]\n",
    "    weights = trained_agent.model.get_weights()\n",
    "    w1, b1 = weights[0], weights[1]\n",
    "    final_w, final_b = weights[2], weights[3]\n",
    "\n",
    "    # --- Build a sub-model to get hidden activations ---\n",
    "    # Instead of using trained_agent.model.input (which may not be defined),\n",
    "    # we create a new input tensor and pass it to our extracted hidden layer.\n",
    "    input_tensor = keras.Input(shape=(5,))\n",
    "    hidden_output = hidden_layer(input_tensor)\n",
    "    hidden_model = keras.Model(inputs=input_tensor, outputs=hidden_output)\n",
    "\n",
    "    # Create the figure.\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    ax.set_xlim(-1, 7)\n",
    "    ax.set_ylim(-1, 5)\n",
    "    ax.axis('off')\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    # Define node sizes.\n",
    "    node_radius_input = 0.2\n",
    "    node_radius_hidden = 0.15   # hidden nodes are drawn a bit smaller.\n",
    "    node_radius_output = 0.2\n",
    "\n",
    "    # Get the number of hidden neurons.\n",
    "    num_hidden = hidden_model.output_shape[-1]\n",
    "\n",
    "    # Define fixed positions for nodes.\n",
    "    layer_positions = {\n",
    "        \"input\": [(0, 4), (0, 3), (0, 2), (0, 1), (0, 0)],  # five inputs\n",
    "        \"hidden\": [(3, i * (4/(num_hidden-1))) for i in range(num_hidden)],\n",
    "        \"output\": [(6, 2), (6, 1), (6, 0)]  # three outputs\n",
    "    }\n",
    "\n",
    "    # Build the normalized state from current slider values.\n",
    "    state = np.array([ball_x, ball_y, paddle_y, ball_dx, ball_dy], dtype=np.float32)\n",
    "    norm_state = trained_agent._normalize_state(state).reshape(1, -1)\n",
    "\n",
    "    # Get full network prediction.\n",
    "    probs = trained_agent.model(norm_state, training=False).numpy().flatten()\n",
    "\n",
    "    # Compute hidden layer activations.\n",
    "    hidden_activations = hidden_model(norm_state, training=False).numpy().flatten()\n",
    "    max_act = hidden_activations.max() if hidden_activations.max() > 0 else 1.0\n",
    "    norm_activations = hidden_activations / max_act  # Normalize to [0, 1]\n",
    "\n",
    "    # Draw input nodes.\n",
    "    for pos in layer_positions['input']:\n",
    "        circle = plt.Circle(pos, node_radius_input, color='lightyellow', ec='k', zorder=5)\n",
    "        ax.add_patch(circle)\n",
    "\n",
    "    # Draw hidden nodes using a blue colormap based on activation.\n",
    "    cmap = plt.get_cmap(\"Blues\")\n",
    "    for i, pos in enumerate(layer_positions['hidden']):\n",
    "        activation = norm_activations[i]\n",
    "        face_color = cmap(0.3 + 0.7 * activation)  # shift so that even low activations are visible.\n",
    "        circle = plt.Circle(pos, node_radius_hidden, color=face_color, ec='k', zorder=5)\n",
    "        ax.add_patch(circle)\n",
    "        # Optionally, display raw activation value.\n",
    "        ax.text(pos[0], pos[1], f\"{hidden_activations[i]:.2f}\",\n",
    "                fontsize=7, ha='center', va='center', zorder=6)\n",
    "\n",
    "    # Draw output nodes.\n",
    "    for pos in layer_positions['output']:\n",
    "        circle = plt.Circle(pos, node_radius_output, color='lightgreen', ec='k', zorder=5)\n",
    "        ax.add_patch(circle)\n",
    "\n",
    "    # Normalize connection line alpha by maximum absolute weight.\n",
    "    max_weight = max(np.abs(w1).max(), np.abs(final_w).max())\n",
    "\n",
    "    # Draw connections from input to hidden using w1.\n",
    "    for i, start_pos in enumerate(layer_positions['input']):\n",
    "        for j, end_pos in enumerate(layer_positions['hidden']):\n",
    "            weight = w1[i, j]\n",
    "            color = 'red' if weight < 0 else 'blue'\n",
    "            alpha = np.abs(weight) / max_weight\n",
    "            ax.plot([start_pos[0] + node_radius_input, end_pos[0] - node_radius_hidden],\n",
    "                    [start_pos[1], end_pos[1]], color=color, alpha=alpha, lw=1)\n",
    "\n",
    "    # Draw connections from hidden to output using final_w.\n",
    "    for j, start_pos in enumerate(layer_positions['hidden']):\n",
    "        for k, end_pos in enumerate(layer_positions['output']):\n",
    "            weight = final_w[j, k]\n",
    "            color = 'red' if weight < 0 else 'blue'\n",
    "            alpha = np.abs(weight) / max_weight\n",
    "            ax.plot([start_pos[0] + node_radius_hidden, end_pos[0] - node_radius_output],\n",
    "                    [start_pos[1], end_pos[1]], color=color, alpha=alpha, lw=1)\n",
    "\n",
    "    # Label the layers.\n",
    "    ax.text(0, 4.5, \"Input Layer\\n(Ball X, Ball Y,\\nPaddle Y,\\nBall DX, Ball DY)\",\n",
    "            ha='center', va='bottom', fontsize=10)\n",
    "    ax.text(3, 4.5, f\"Hidden Layer\\n({num_hidden} Neurons)\",\n",
    "            ha='center', va='bottom', fontsize=10)\n",
    "    ax.text(6, 4.5, \"Output Layer\\n(Up, Stay, Down)\",\n",
    "            ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    # Display network prediction probabilities.\n",
    "    pred_text = (f\"Network Prediction:\\n\"\n",
    "                 f\"  Up: {probs[0]*100:.1f}%\\n\"\n",
    "                 f\"  Stay: {probs[1]*100:.1f}%\\n\"\n",
    "                 f\"  Down: {probs[2]*100:.1f}%\")\n",
    "    ax.text(6, -0.5, pred_text, ha='center', va='top',\n",
    "            bbox=dict(facecolor='white', alpha=0.9), fontsize=12)\n",
    "\n",
    "    plt.title(\"Network Architecture and Hidden Neuron Activations\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- Create slider widgets (ensure that WIDTH, HEIGHT, BALL_SPEED, PADDLE_HEIGHT are defined) ---\n",
    "slider_ball_x = widgets.FloatSlider(min=0, max=WIDTH, value=WIDTH/2, description=\"Ball X\",\n",
    "                                    layout=widgets.Layout(width='300px'))\n",
    "slider_ball_y = widgets.FloatSlider(min=0, max=HEIGHT, value=HEIGHT/2, description=\"Ball Y\",\n",
    "                                    layout=widgets.Layout(width='300px'))\n",
    "slider_paddle_y = widgets.FloatSlider(min=0, max=HEIGHT-PADDLE_HEIGHT, value=160, description=\"Paddle Y\",\n",
    "                                      layout=widgets.Layout(width='300px'))\n",
    "slider_ball_dx = widgets.FloatSlider(min=-BALL_SPEED, max=BALL_SPEED, value=BALL_SPEED,\n",
    "                                     description=\"Ball DX\", layout=widgets.Layout(width='300px'))\n",
    "slider_ball_dy = widgets.FloatSlider(min=-BALL_SPEED, max=BALL_SPEED, value=BALL_SPEED,\n",
    "                                     description=\"Ball DY\", layout=widgets.Layout(width='300px'))\n",
    "\n",
    "sliders_box = VBox([slider_ball_x, slider_ball_y, slider_paddle_y, slider_ball_dx, slider_ball_dy])\n",
    "\n",
    "# --- Create the interactive widget ---\n",
    "interactive_plot = interactive(visualize_trained_network,\n",
    "                               ball_x=slider_ball_x,\n",
    "                               ball_y=slider_ball_y,\n",
    "                               paddle_y=slider_paddle_y,\n",
    "                               ball_dx=slider_ball_dx,\n",
    "                               ball_dy=slider_ball_dy)\n",
    "\n",
    "display(HBox([sliders_box, interactive_plot.children[-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workhorse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
